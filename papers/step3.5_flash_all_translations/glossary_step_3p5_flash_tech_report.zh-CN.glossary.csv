source,target,tgt_lng
concatenation,连接,
MTP head 2,MTP 头 2,
(Shared) Embedding,共享嵌入,
Gated SWA,门控 SWA,
Block,块,
H-Norm,H范数,
E-Norm,E范数,
Linear,线性,
Main Stream,主流,
MTP module 1,MTP 模块 1,
Loss,损失,
Steps,步数,
Layer 38,第38层,
Solid,实线,
Dashed,虚线,
max,最大值,
Median,中位数,
WinoGrande,WinoGrande,
5-shot,5-shot,
GPQA,GPQA,
4-shot,4-shot,
HumanEval,HumanEval,
3-shot,3-shot,
MMLU-Redux,MMLU-Redux,
MMLU-Pro,MMLU-Pro,
HellaSwag,HellaSwag,
10-shot,10-shot,
Base,Base,
Activated Params,激活参数,
11B,11B,
15B,15B,
32B,32B,
37B,37B,
Total Params,总参数,
196B,196B,
309B,309B,
GPQA-Diamond,GPQA-Diamond,
13.87%,13.87%,
General,通用,
0.80B,0.80B,
Code Agent,代码代理,
0.90B,0.90B,
Tool-use,工具使用,
0.76B,0.76B,
Search Agent,搜索代理,
0.50B,0.50B,
8.75%,8.75%,
BBH,BBH,
MMLU,MMLU,
MBPP,MBPP,
C-EVAL,C-EVAL,
CMMLU,CMMLU,
Avg.,平均,
Sink Token,汇流标记,
Head-wise Gate,逐头门控,
S3F1,S3F1,
NVIDIA H800,NVIDIA H800,
NVLink,NVLink,
NVSwitch,NVSwitch,
RoCE,RoCE,
Step 3.5 Flash,Step 3.5 Flash,
Steptron,Steptron,
PyTorch,PyTorch,
Megatron-LM,Megatron-LM,
强化学习 (RL),强化学习 (RL),
MIS-PO,MIS-PO,
PPO,PPO,
Rotary Position Embeddings,旋转位置嵌入,
Muon optimizer,Muon优化器,
MoE router,MoE路由器,
EP-group balance loss,EP组平衡损失,
SFT training,SFT训练,
MTP loss,MTP损失,
Rotary Position Embeddings (RoPE),旋转位置嵌入（RoPE）,
Metropolis Independence Sampling,Metropolis独立采样,
inference policy,推理策略,
training policy,训练策略,
binary masking,二进制掩码,
off-distribution samples,离分布样本,
gradient variance,梯度方差,
IMO-AnswerBench,IMO-AnswerBench,
LiveCodeBench-v6,LiveCodeBench-v6,
CF-Div2-Stepfun-cpp,CF-Div2-Stepfun-cpp,
τ2-Bench,τ2-Bench,
Arena-Hard-v2.0,Arena-Hard-v2.0,
RepoQA,RepoQA,
MRCR-8needle,MRCR-8needle,
FRAMES-Oracle,FRAMES-Oracle,
355B,355B,
671B,671B,
1043B,1043B,
88.2†,88.2†,
88.7†,88.7†,
87.4†,87.4†,
87.8†,87.8†,
Benchmark,基准测试,
Shots,样本,
MiMo-V2 Flash,MiMo-V2 闪存,
GLM-4.5,GLM-4.5,
DeepSeek V3.1,DeepSeek V3.1,
DeepSeek V3.2,DeepSeek V3.2,
Exp Base,实验基础,
Kimi-K2,Kimi-K2,
MultiChallenge,MultiChallenge,
IFBench,IFBench,
SuperGPQA,SuperGPQA,
42.3,42.3,
43.6,43.6,
SimpleQA,SimpleQA,
26.3,26.3,
27.0,27.0,
MATHEMATICS,数学,
GSM8K,GSM8K,
8-shot,8-shot,
91.4,91.4,
Vanilla,Vanilla,
PaCoRe,PaCoRe,
MiniMax,MiniMax,
M2.1,M2.1,
MiMo V2,MiMo V2,
Flash,Flash,
GLM,GLM,
DeepSeek,DeepSeek,
V3.2,V3.2,
Kimi K2.5,Kimi K2.5,
Gemini,Gemini,
CHINESE,中文,
C-SimpleQA,C-SimpleQA,
Step 3.5,步骤3.5,
loss spike,损失尖峰,
loss mask,损失掩码,
Muon,Muon,
polar-factor iteration,极因子迭代,
reduced precision,降低精度,
expert-side collapse,专家端崩溃,
router dispatch statistics,路由调度统计,
loss reduction,损失减少,
orthogonalization approximation,正交化近似,
Polar Express,Polar Express,
Newton–Schulz (NS) iteration,牛顿-舒尔茨（NS）迭代,
safety scaling,安全缩放,
bfloat16,bfloat16,
cumulative error,累积误差,
float16,float16,
DeepSeek-R1,DeepSeek-R1,
structured report generation pipeline,结构化报告生成管道,
R E S E A R C HR U B R I C S,R E S E A R C HR U B R I C S,
Tool-Use Template Design,工具使用模板设计,
Tool-Use Template,工具使用模板,
JSON,JSON,
XML,XML,
Scalable Code Agent Infrastructure,可扩展代码代理基础设施,
Session-Router,会话路由器,
Kubernetes,Kubernetes,
Tmux,Tmux,
Open-Hands,Open-Hands,
SWE-agent,SWE-agent,
Terminus-2,Terminus-2,
Kilocode,Kilocode,
Roocode,Roocode,
ClaudeCode,ClaudeCode,
CF-Div2-Stepfun,CF-Div2-Stepfun,
LiveCodeBench,LiveCodeBench,
SuperG-PQA,SuperG-PQA,
MATH,MATH,
HumanEval+,HumanEval+,
MBPP+,MBPP+,
MultiPL-E,MultiPL-E,
binary indicator function,二元指示函数,
token level,token级别,
trajectory level,轨迹级别,
geometric mean ratio,几何平均比率,
actor loss,演员损失,
trust-region constraint,信任区域约束,
ablation study,消融研究,
actor gradient norm,演员梯度范数,
Truncation-Aware Value Bootstrapping,截断感知值引导,
context-length truncation,上下文长度截断,
Routing Confidence,路由置信度,
MoE architectures,MoE架构,
Routing Confidence as a Stability Proxy,作为稳定性代理的路由置信度,
RL stability,RL稳定性,
Router Replay,Router Replay,
on-policy updates,在线策略更新,
RL with verifiable rewards (RLVR),具有可验证奖励的RL (RLVR),
ARC-AGI-1,ARC-AGI-1,
HLEtext,HLEtext,
81.0,81.0,
81.6,81.6,
74.6,74.6,
75.6,75.6,
89.0,89.0,
0-shot,0-shot,
64.6,64.6,
67.7,67.7,
GSPO,GSPO,
GitHub,GitHub,
LLMs,大语言模型,
PRs,拉取请求,
issues,问题,
commits,提交,
repository popularity,仓库流行度,
content quality,内容质量,
issue descriptions,问题描述,
5-million-sample foundation,500万样本基础,
First 30B tokens,30B tokens,
Layers,层,
Dimension,维度,
Leading Dense Layers,主导密集层,
Routed Experts,路由专家,
Active Experts,活跃专家,
Shared Experts,共享专家,
Load Balancing Method,负载均衡方法,
Loss Free [64],无损[64],
Attention module,注意力模块,
GQA8,GQA8,
Sequence Length,序列长度,
Vocab Size,词汇量大小,
Token Efficiency,Token效率,
frontier-level intelligence,前沿级智能,
Gemini 3.0 Pro,Gemini 3.0 Pro,
generation trajectories,生成轨迹,
prune and compress the thinking,修剪和压缩思考,
competitive performance,竞争性能,
EfficientUniversal Mastery,高效通用掌握,
generalist versatility,通用多功能性,
deep domain expertise,深度领域专业知识,
on-policy distillation,在线策略蒸馏,
internalize expert behaviors,内化专家行为,
sample efficiency,样本效率,
codeforces.com,codeforces.com,
huggingface.co/datasets/openai/mrcr,huggingface.co/datasets/openai/mrcr,
dense model,密集模型,
actorgradient norm,动作梯度范数,
Moe model,MoE模型,
accelerated convergence,加速收敛,
training-inference discrepancy,训练-推理差异,
density ratio,密度比,
rollout policy,滚动策略,
pre-update policy snapshot,预更新策略快照,
Dense,密集,
MoE,MoE,
Generalized Importance Sampling,广义重要性采样,
token-level importance sampling ratio,token级重要性采样比率,
trajectory-level ratios,轨迹级比率,
Chain-of-Thought,思维链,
LLM-based,基于大语言模型的,
gpt-oss-120b,gpt-oss-120b,
CoT-prompts,CoT-prompts,
Tool-integrated PaCoRe,工具集成PaCoRe,
Kimi K2-Thinking,Kimi K2-Thinking,
GLM-4.7,GLM-4.7,
MiniMax M2.1,MiniMax M2.1,
Claude Opus 4.5,Claude Opus 4.5,
Anchor-Based,基于锚点的,
GPT-5.2,GPT-5.2,
Reference Response,参考响应,
Anchor,锚点,
Usefulness,实用性,
Logic,逻辑性,
Instruction Following,指令遵循,
Tone,语气,
Hybrid LLM-as-a-Judge,混合LLM作为裁判,
Minimax-M2.1,Minimax-M2.1,
system prompt,系统提示,
question prompt,问题提示,
MMLU-Pro dataset,MMLU-Pro数据集,
binary choices,二进制选择,
Claude Sonnet,Claude Sonnet,
Table 11,表11,
Baseline Score,基线分数,
▲Performance Gain,▲性能提升,
Avg Gain,平均提升,
256K setting,256K设置,
agentic competence,代理能力,
parametric memorization,参数记忆,
tool-usage gain,工具使用提升,
GAIA,GAIA,
xbench-DeepSearch,xbench-DeepSearch,
HLE,HLE,
SWE-Bench Verified,SWE-Bench 验证版,
SWE-Bench Multilingual,SWE-Bench 多语言版,
OpenHands,OpenHands,
CodeAct Agent framework,CodeAct 代理框架,
execute_bash,execute_bash,
str_replace_editor,str_replace_editor,
finish,finish,
think,think,
compiled languages,编译型语言,
Artifactory repository,Artifactory 仓库,
Terminus 2 framework,Terminus 2 框架,
STEM,STEM,
puzzle benchmarks,谜题基准测试,
AIME 2025,AIME 2025,
HMMT 2025 Feb.,HMMT 2025 二月,
HMMT 2025 Nov.,HMMT 2025 十一月,
Batch Size,批大小,
Weight Decay,权重衰减,
Partial RoPE,部分RoPE,
MTP,MTP,
IFEval,IFEval,
WildBench,WildBench,
Arena-Hard,Arena-Hard,
RULER,RULER,
Long-Bench v2,Long-Bench v2,
HELMET,HELMET,
GSM-Infinite,GSM-Infinite,
FRAMES,FRAMES,
S3F1 layout,S3F1布局,
full-attention baseline,全注意力基线,
general pretraining benchmarks,通用预训练基准,
SFT quality,SFT质量,
SFT Avg,SFT平均,
SWA query heads,SWA查询头,
SFT Reasoning,SFT推理,
SFT Code,SFT代码,
head-wise gated attention,头门控注意力,
sink token metric,汇流标记指标,
BrowseComp,BrowseComp,
Δtool,Δtool,
HMMT 2025,HMMT 2025,
tool-integrated reasoning,工具集成推理,
parallel reasoning,并行推理,
Python interpreter,Python解释器,
sandbox,沙盒,
AIME,AIME,
HMMT,HMMT,
IMO,IMO,
GPQ,GPQ,
ARC,ARC,
Accuracy (avg@8),平均@8准确率,
Codeforces C++,Codeforces C++,
Model,模型,
pass@8 Rating,通过@8评级,
external search engines,外部搜索引擎,
scientific data,科学数据,
open-source checkers,开源检查器,
real-world constraints,现实世界约束,
open-source,开源,
synthetic,合成,
user trajectories,用户轨迹,
high-fidelity dataset,高保真数据集,
billion-token scale,百亿级token规模,
execution-driven data generation framework,执行驱动数据生成框架,
intelligent agents,智能体,
synthetic pipelines,合成管道,
data inconsistency,数据不一致性,
verifiability,可验证性,
model hallucinations,模型幻觉,
random exploration,随机探索,
model-based simulation,基于模型的模拟,
finite state machine,有限状态机,
FSM,有限状态机,
sample–execute–verify loop,采样-执行-验证循环,
rejection sampling,拒绝采样,
candidate trajectories,候选轨迹,
real environments,真实环境,
deterministic feedback,确定性反馈,
atomic intents,原子意图,
compositionally combining,组合式结合,
high-quality trajectories,高质量轨迹,
code agents,代码代理,
closed-loop intervention,闭环干预,
verifiable environment construction,可验证环境构建,
solution generation,解决方案生成,
SWE-factory framework,SWE-factory框架,
cross-task memory pool,跨任务内存池,
loop-detection mechanism,循环检测机制,
environment-building success rate,环境构建成功率,
positive feedback loop,正向反馈循环,
dense supervision,密集监督,
environment construction trajectories,环境构建轨迹,
shell commands,shell命令,
error recovery,错误恢复,
transient failures,瞬态故障,
redundant execution patterns,冗余执行模式,
final resolution,最终解决方案,
bootstrapped environments,引导环境,
bidirectional transfer,双向迁移,
construction expertise,构建专业知识,
coding performance,编码性能,
DockSmith,DockSmith,
verified environments,验证环境,
GitHub repositories,GitHub仓库,
programming languages,编程语言,
real-world scenarios,现实世界场景,
generalist code agents,通用代码代理,
search and research agents,搜索和研究代理,
BrowseComp-ZH,BrowseComp-ZH,
xbench,xbench,
DeepSearch-2505,DeepSearch-2505,
DeepSearch-2510,DeepSearch-2510,
AGENT,AGENT,
ΔAVG@3,ΔAVG@3,
METRIC,指标,
PASS RATE,通过率,
OpenAI,OpenAI,
Openai o3-mini,Openai o3-mini,
gpt-5,gpt-5,
Haolong Yan,闫浩龙,
Jia Wang,王嘉,
Xin Huang,黄欣,
Yeqing Shen,沈叶青,
Ziyang Meng,孟子阳,
Zhimin Fan,范志明,
Kaijun Tan,谭凯俊,
Jin Gao,高金,
Lieyu Shi,石立宇,
Mi Yang,杨米,
Step-gui technical report,Step-gui 技术报告,
arXiv preprint arXiv:2512.15431,arXiv 预印本 arXiv:2512.15431,
Agent System,智能体系统,
Score,分数,
Gemini DeepResearch,Gemini DeepResearch,
OpenAI DeepResearch,OpenAI DeepResearch,
Kimi Researcher,Kimi Researcher,
MiniMax Agent Pro,MiniMax Agent Pro,
Qwen DeepResearch,Qwen DeepResearch,
R E S E A R C HR U B R I C S benchmar,R E S E A R C HR U B R I C S 基准测试,
LLM,大语言模型,
ReAct framework,ReAct 框架,
batch_web_surfer,批量网页浏览器,
file,文件,
Table 12,表12,
Step 3.5 Flash w. Python,步骤3.5闪存+Python,
Tool-integrated Parallel Reasoning,工具集成并行推理,
standard LLM message interface,标准LLM消息接口,
existing agentic frameworks,现有代理框架,
multi-turn tool interaction,多轮工具交互,
state-aware input serialization protocol,状态感知输入序列化协议,
Table 13,表13,
agentic loops,代理循环,
standard reasoning baseline,标准推理基线,
interactive feedback,交互反馈,
agentic test-time scaling,代理测试时扩展,
Benchmark w. Python,带Python的基准测试,
Step 3.5 Flash + PaCoRe,步骤3.5闪存+PaCoRe,
evaluationmethod,评估方法,
repeat@64,repeat@64,
detailedsystem prompts,详细系统提示,
LLM judge,大语言模型裁判,
BrowseComp (w. Ctx Manage),带上下文管理的浏览组件,
discard-all methodology,丢弃全部方法,
full BrowseComp dataset,完整浏览组件数据集,
context length threshold,上下文长度阈值,
agent discards its entire context,代理丢弃其整个上下文,
operational loop,操作循环,
maximum iteration constraint,最大迭代约束,
context management strategies,上下文管理策略,
200 instances,200个实例,
BrowseC-omp,浏览组件,
Summary,摘要,
Keep-first&lastK,保留首尾K,
Multi-agent orchestration,多代理协调,
Table 17,表17,
model demonstrates robust adaptability,模型展示出强大的适应性,
single-agent strategies,单代理策略,
test-time pass@k strategy,测试时pass@k策略,
self-verified path,自我验证路径,
real steps,实际步数,
inference cost,推理成本,
context management,上下文管理,
R E S E A R C H R U B R I C S,研究问题集,
domain-diverse research tasks,领域多样化的研究任务,
"expert-written, fine-grained scoring criteria",专家编写的细粒度评分标准,
factual accuracy,事实准确性,
reasoning soundness,推理合理性,
clarity,清晰度,
commercial agent systems,商业代理系统,
ReAct agents,ReAct代理,
Iz Beltagy,Iz Beltagy,
Matthew E. Peters,Matthew E. Peters,
Arman Cohan,Arman Cohan,
Longformer,Longformer,
Gemma Team,Gemma Team,
Aishwarya Kamath,Aishwarya Kamath,
Johan Ferret,Johan Ferret,
Yaniv Leviathan,Yaniv Leviathan,
Matan Kalman,Matan Kalman,
Yossi Matias,Yossi Matias,
Fast inference from transformers via speculative decoding,基于推测解码的 transformer 快速推理,
"Proceedings of the 40th International Conference on Machine Learning, ICML’23",第 40 届机器学习国际会议，ICML’23,
Imanol Schlag,Imanol Schlag,
Kazuki Irie,Kazuki Irie,
Jürgen Schmidhuber,Jürgen Schmidhuber,
Linear transformers are secretly fast weight programmers,线性 transformer 暗中是快速权重程序员,
International conference on machine learning,机器学习国际会议,
Jikai Wang,Jikai Wang,
Yi Su,Yi Su,
Juntao Li,Juntao Li,
Qingrong Xia,Qingrong Xia,
Zi Ye,Zi Ye,
Xinyu Duan,Xinyu Duan,
Zhefeng Wang,Zhefeng Wang,
Min Zhang,Min Zhang,
Opt-tree,Opt-tree,
Speculative decoding with adaptive draft tree structure,具有自适应草稿树结构的推测解码,
Transactions of the Association for Computational Linguistics,计算语言学协会汇刊,
Yunfan Xiong,Yunfan Xiong,
Ruoyu Zhang,Ruoyu Zhang,
Yanzeng Li,Yanzeng Li,
Lei Zou,Lei Zou,
Dyspec,Dyspec,
Faster speculative decoding with dynamic token tree structure,具有动态标记树结构的更快推测解码,
World Wide Web,万维网,
Haoran You,Haoran You,
Yichao Fu,Yichao Fu,
Zheng Wang,Zheng Wang,
Amir Yazdanbakhsh,Amir Yazdanbakhsh,
Yingyan (Celine) Lin,Yingyan (Celine) Lin,
"Proceedings of the 41st International Conference on Machine Learning, ICML’24",第 41 届机器学习国际会议，ICML’24,
Joshua Ainslie,Joshua Ainslie,
James Lee-Thorp,James Lee-Thorp,
Michiel de Jong,Michiel de Jong,
Yury Zemlyanskiy,Yury Zemlyanskiy,
Federico Lebron,Federico Lebron,
Sumit Sanghai,Sumit Sanghai,
GQA,GQA,
Training generalized multi-query transformer models from multi-head checkpoints,从多头检查点训练广义多查询 transformer 模型,
Houda Bouamor,Houda Bouamor,
Juan Pino,Juan Pino,
Kalika Bali,Kalika Bali,
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,2023 年自然语言处理经验方法会议论文集,
Yuhui Li,Yuhui Li,
Fangyun Wei,Fangyun Wei,
Chao Zhang,Chao Zhang,
Hongyang Zhang,Hongyang Zhang,
EAGLE,EAGLE,
Speculative sampling requires rethinking feature uncertainty,推测采样需要重新思考特征不确定性,
Thomas Mesnard,Thomas Mesnard,
Cassidy Hardin,Cassidy Hardin,
Robert Dadashi,Robert Dadashi,
Surya Bhupatiraju,Surya Bhupatiraju,
Shreya Pathak,Shreya Pathak,
Laurent Sifre,Laurent Sifre,
Morgane Rivière,Morgane Rivière,
Mihir Sanjay Kale,Mihir Sanjay Kale,
Juliette Love,Juliette Love,
Team Cohere,Team Cohere,
Aakanksha,Aakanksha,
Arash Ahmadian,Arash Ahmadian,
Marwan Ahmed,Marwan Ahmed,
Command a,Command a,
An enterprise-ready large language model,企业级大语言模型,
Guangxuan Xiao,Guangxuan Xiao,
Yuandong Tian,Yuandong Tian,
Beidi Chen,Beidi Chen,
Song Han,Song Han,
Mike Lewis,Mike Lewis,
Efficient streaming language models with attention sinks,具有注意力汇流的流式语言模型,
The Twelfth International Conference on Learning Representations,第 12 届学习表示国际会议,
Mingjie Sun,Mingjie Sun,
Xinlei Chen,Xinlei Chen,
J Zico Kolter,J Zico Kolter,
Zhuang Liu,Zhuang Liu,
Massive activations in large language models,大语言模型中的大规模激活,
First Conference on Language Modeling,语言建模第一次会议,
Xiangming Gu,Xiangming Gu,
Tianyu Pang,Tianyu Pang,
Chao Du,Chao Du,
Qian Liu,Qian Liu,
Fengzhuo Zhang,Fengzhuo Zhang,
Cunxiao Du,Cunxiao Du,
Ye Wang,Ye Wang,
Min Lin,Min Lin,
When attention sink emerges in language models: An empirical view,当注意力汇流在语言模型中出现：一种经验性观点,
The Thirteenth International Conference on Learning Representations,第 13 届学习表示国际会议,
John Jumper,John Jumper,
Richard Evans,Richard Evans,
Alexander Pritzel,Alexander Pritzel,
Tim Green,Tim Green,
Michael Figurnov,Michael Figurnov,
Olaf Ronneberger,Olaf Ronneberger,
Kathryn Tunyasuvunakool,Kathryn Tunyasuvunakool,
Russ Bates,Russ Bates,
Augustin Žídek,Augustin Žídek,
Anna Potapenko,Anna Potapenko,
Alex Bridgland,Alex Bridgland,
Clemens Meyer,Clemens Meyer,
Simon A. A. Kohl,Simon A. A. Kohl,
Andrew J. Ballard,Andrew J. Ballard,
Andrew Cowie,Andrew Cowie,
Bernardino Romera- Paredes,Bernardino Romera- Paredes,
Stanislav Nikolov,Stanislav Nikolov,
Rishub Jain,Rishub Jain,
Jonas Adler,Jonas Adler,
Trevor Back,Trevor Back,
Stig Petersen,Stig Petersen,
David Reiman,David Reiman,
Ellen Clancy,Ellen Clancy,
Michal Zielinski,Michal Zielinski,
Martin Steinegger,Martin Steinegger,
Michalina Pacholska,Michalina Pacholska,
Tamas Berghammer,Tamas Berghammer,
Sebastian Bodenstein,Sebastian Bodenstein,
David Silver,David Silver,
Oriol Vinyals,Oriol Vinyals,
Andrew W. Senior,Andrew W. Senior,
Koray Kavukcuoglu,Koray Kavukcuoglu,
Pushmeet Kohli,Pushmeet Kohli,
Demis Hassabis,Demis Hassabis,
Highly accurate protein structure prediction with AlphaFold,使用 AlphaFold 的高精度蛋白质结构预测,
Nature,自然,
AlphaFold,AlphaFold,

source,target,tgt_lng
Repo-level Code Training,仓库级代码训练,
cross-file dependency,跨文件依赖,
model’s software engineering capability,模型的软件工程能力,
YYYY-MM-DD,YYYY-MM-DD格式,
city,城市,
Shanghai,上海,
date,日期,
Pre-training,预训练,
32K,32K,
128K,128K,
Mid-training,中训练,
General,通用,
Corpus,语料库,
(15T),(15T),
Code &,代码 &,
Reasoning,推理,
12B,12B,
37B,37B,
32B,32B,
Dense Layers,密集层,
MoE Layers,MoE层,
MTP Layers,MTP层,
Hidden Dim,隐藏维度,
Dense Intermediate Dim,密集中间维度,
MoE Intermediate Dim,MoE中间维度,
Attention Head Dim,注意力头维度,
Attention Heads,注意力头,
Key-Value Heads,键值头,
Experts (total),专家（总数）,
Continual,持续学习,
Repo-Level,仓库级,
Code Data,代码数据,
500B,500B,
Synthetic,合成,
Reasoning Data,推理数据,
Long Context & Agent Data,长上下文与代理数据,
100B,100B,
General RL,通用强化学习,
multi-source feedback system,多源反馈系统,
rule-based feedback,基于规则的反馈,
human feedback,人类反馈,
model-based feedback,基于模型的反馈,
hybrid framework,混合框架,
RLHF,RLHF,
RLAIF,RLAIF,
LLM Performance Evaluation,大语言模型性能评估,
Agentic,自主,
Coding,编码,
MMLU-Pro,MMLU-Pro,
AIME 24,AIME 24,
MATH-500,MATH-500,
SciCode,SciCode,
GPQA,GPQA,
HLE,HLE,
LCB (2407-2501),LCB (2407-2501),
SWE-Bench Verified,SWE-Bench验证,
Terminal-Bench,Terminal-Bench,
TAU-Bench,TAU-Bench,
BFCL V3,BFCL V3,
BrowseComp,BrowseComp,
GLM-4.5,GLM-4.5,
ARC Foundation Models,ARC基础模型,
GLM-4.5 Team,GLM-4.5团队,
AIME24 AVG@32,AIME24 AVG@32,
RL with Difficulty-based Curriculum Learning,基于难度分层课程学习的强化学习,
Moderate difficulty data,中等难度数据,
Extremely difficulty data,极高难度数据,
samples_per_prompt,每提示样本数,
16K Context,16K上下文,
32K Context,32K上下文,
48K Context,48K上下文,
64K Context,64K上下文,
BBH,BBH,
MMLU,MMLU,
HellaSwag,HellaSwag,
PIQA,PIQA,
TriviaQA,TriviaQA,
EvalPlus,EvalPlus,
Pass@1,Pass@1,
LiveCodeBench-Base,LiveCodeBench-Base,
Math,数学,
GSM8K,GSM8K,
Chinese,中文,
CLUEWSC,CLUEWSC,
C-Eval,C-Eval,
AA-Index,AA-索引,
GLM-,GLM-,
SWE-bench,SWE-bench,
GPT-4.1,GPT-4.1,
Claude,Claude,
Opus 4,Opus 4,
Sonnet 4,Sonnet 4,
Gemini,Gemini,
DeepSeek-R1-0528,DeepSeek-R1-0528,
Kimi-K2,Kimi-K2,
TextGeneration,文本生成,
Subjective QA,主观问答,
Berkeley Function Calling Leaderboard,伯克利函数调用排行榜,
4.5-Air,4.5-Air,
mini,mini,
DeepSeek,DeepSeek,
R1 0528,R1 0528,
Kimi K2,Kimi K2,
OpenHands,OpenHands,
2LiveCodeBench,2LiveCodeBench,
7/1/2024,2024年7月1日,
1/1/2025,2025年1月1日,
CC-Bench,CC-Bench,
Claude Code5,Claude Code5,
Claude Sonnet 4,Claude Sonnet 4,
Qwen3-Coder,Qwen3-Coder,
Win,赢,
Tie,平,
Lose,输,
40.4%,40.4%,
50%,50%,
9.6%,9.6%,
53.9%,53.9%,
English,英语,
Mathematics,数学,
Objective QA,客观问答,
Text Generation,文本生成,
Human Evaluation Scores,人工评估分数,
Subjective,主观,
Objective,客观,
Text Proc.,文本处理,
Subj. QA,主观问答,
Obj. QA,客观问答,
Logic,逻辑,
Code,代码,
2.5 Pro,2.5 Pro,
Kimi,Kimi,
Grok 4,Grok 4,
TAU-Retail,TAU-Retail,
TAU-Airline,TAU-Airline,
Average,平均,
Claude Opus 4,Claude Opus 4,
OpenAI o3,OpenAI o3,
o4-mini,o4-mini,
fish,鱼,
calico,三花猫,
Qwen-MT-plus,Qwen-MT-plus,
Qwen-MT-turbo,Qwen-MT-turbo,
Seed-X,Seed-X,
Pro,Pro,
0528,0528,
Qwen3,Qwen3,
235B,235B,
2507,2507,
Grok,Grok,
MMLU Pro,MMLU Pro,
AIME,AIME,
500,500,
LCB,LCB,
MATH 500,MATH 500,
Humanity’s Last Exam,人类最后考试,
LiveCodeBench,LiveCodeBench,
LLM,大语言模型,
GPT-4o,GPT-4o,
Artificial Analysis,人工智能分析,
Reasoning Benchmarks,推理基准测试,
V3 0324,V3 0324,
SimpleQA,SimpleQA,
IFEval,IFEval,
SysBench,SysBench,
MultiChallenge,MultiChallenge,
MMLU (EM),MMLU (EM),
SimpleQA (Correct),SimpleQA (Correct),
IFEval (Prompt Strict),IFEval (Prompt Strict),
SysBench (ISR),SysBench (ISR),
GLM-4.5 (355B),GLM-4.5 (355B),
Gemini 2.5 Pro,Gemini 2.5 Pro,
GLM-4.5-Air,GLM-4.5-Air,
yyds,yyds,
永远的神,永远的神,
CanonEF70-300mm f/4-5.6IS USM,CanonEF70-300mm f/4-5.6IS USM,
AIME24,AIME24,
Multi-stage Training,多阶段训练,
Single-stage Training,单阶段训练,
16K 32K,16K 32K,
32K 48K,32K 48K,
48K 64K,48K 64K,
two-stage difficulty-based curriculum,两阶段难度递增课程,
RL,强化学习,
blueline,蓝线,
red line,红线,
baseline,基线,
"pass@8=0, pass@512>0",通过率@8=0，通过率@512>0,
moderate-difficulty problems,中等难度问题,
plateaus,平台期,
single-stage at 64K,64K单阶段,
multi-stage with progressively increasing length,逐步增加长度的多阶段,
performance drop,性能下降,
reward variance,奖励方差,
gradient signal,梯度信号,
training efficiency,训练效率,
two-stage approach,两阶段方法,
performance ceiling,性能上限,
signal quality,信号质量,
noise,噪声,
pool with verified correct answers,已验证正确答案的池,
Single-Stage RL at 64K Output Length,64K输出长度单阶段RL,
maximum output lengths,最大输出长度,
Supervised Fine-Tuning (SFT),监督微调（SFT）,
unlearn,遗忘,
long-context capabilities,长上下文能力,
average output length,平均输出长度,
degradation,退化,
final 64K-length RL stage,最终64K长度RL阶段,
full 64K-length,完整64K长度,
Aohan Zeng,曾澳瀚,
Xin Lv,吕欣,
Qinkai Zheng,郑勤凯,
Zhenyu Hou,侯振宇,
Jie Tang,唐杰,
Yuxiao Dong,董宇潇,
Juanzi Li,李娟子,
Hongning Wang,王洪宁,
Minlie Huang,黄敏丽,
Bin Xu,徐斌,
Jidong Zhai,翟继东,
Wenguang Chen,陈文广,
Beijing,北京,
Tianjin,天津,
Hangzhou,杭州,
Zhuhai,珠海,
Chengdu,成都,
customers,客户,
community developers,社区开发者,
pipeline,管道,
throughput,吞吐量,
agent interactions,智能体交互,
agent frameworks,智能体框架,
HTTP endpoint interface,HTTP端点接口,
centralized data pool,集中式数据池,
message-list format,消息列表格式,
trajectories,轨迹,
RL training process,强化学习训练过程,
heterogeneous agent frameworks,异构智能体框架,
"customizable, task-specific filtering",可定制、任务特定的过滤,
dynamic sampling strategies,动态采样策略,
long-agentic RL,长智能体强化学习,
long-horizon rollouts,长时程轨迹,
base model,基础模型,
GLM-4.5-Base,GLM-4.5-Base,
instruction data,指令数据,
internal evaluation framework,内部评估框架,
benchmarks,基准测试,
Qwen3-235B,Qwen3-235B,
Llama4-Maverick,Llama4-Maverick,
DeepSeek-V3,DeepSeek-V3,
Llama,Llama,
Michelan- gelo,Michelan- gelo,
Qwenlong-l1,Qwenlong-l1,
mixture-of-experts,专家混合,
Beyond the 80/20 rule,超越80/20法则,
Measuring short-form factuality in large language models,大型语言模型中的短形式事实性测量,
Agentgym,Agentgym,
Dapo,Dapo,
Glm-130b,Glm-130b,
Zhang,张,
Lei,雷,
Wu,吴,
Sun,孙,
Huang,黄,
Long,龙,
Liu,刘,
Tang,唐,
Safetybench,Safetybench,
large language models,大语言模型,
multiple choice questions,多项选择题,
arXiv preprint arXiv:2309.07045,arXiv预印本arXiv:2309.07045,
Zhou,周,
Lu,陆,
Mishra,米什拉,
Brahma,布拉马,
Basu,巴苏,
Luan,阮,
instruction-following evaluation,指令遵循评估,
arXiv preprint arXiv:2311.07911,arXiv预印本arXiv:2311.07911,

# GLM-4.5: Agentic, Reasoning, and Coding (ARC)Foundation Models

GLM-4.5 Team

Zhipu AI & Tsinghua University

(For the complete list of authors, please refer to the Contribution section)

# Abstract

We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large languagemodel with 355B total parameters and 32B activated parameters, featuring ahybrid reasoning method that supports both thinking and direct response modes.Through multi-stage training on 23T tokens and comprehensive post-trainingwith expert model iteration and reinforcement learning, GLM-4.5 achieves strongperformance across agentic, reasoning, and coding (ARC) tasks, scoring $7 0 . 1 \%$ onTAU-Bench, $9 1 . 0 \%$ on AIME 24, and $6 4 . 2 \%$ on SWE-bench Verified. With muchfewer parameters than several competitors, GLM-4.5 ranks 3rd overall amongall evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5(355B parameters) and a compact version, GLM-4.5-Air (106B parameters), toadvance research in reasoning and agentic AI systems. Code, models, and moreinformation are available at https://github.com/zai-org/GLM-4.5.


LLM Performance Evaluation: Agentic, Reasoning, and Coding Benchmarks


![](images/5391b77d2dffe33d576f7876afc22369de009c6137917ef2fb726898bcaf37ab.jpg)



12 benchmarks: MMLU-Pro, AIME 24, MATH-500, SciCode, GPQA, HLE, LCB (2407-2501), SWE-Bench Verified, Terminal-Bench, TAU-Bench, BFCL V3, BrowseComp


![](images/adbf289f0da9d0ecac8e8f0595837e7a7fed632d4dfac90c63ea193b47d215a1.jpg)



Agentic



Agentic Benchmarks: TAU-bench, BFCL v3 (Full),BrowseComp


![](images/a5dc5da2b8639050fd04e32edefae91bfdfff5600d45cae959d546026d0e7150.jpg)



Reasoning



Reasoning Benchmarks: MMLU-Pro, AIME 24, MATH 500,SciCode, GPQA, HLE, LCB (2407-2501)


![](images/a22fdaaa738239b6818e0710639382eb517d59cfa7644716b17983517a8dfc67.jpg)



Coding



Coding Benchmarks: SWE-Bench Verified, Terminal-bench


![](images/9782be734f403837fdf58947ed144c92c9b16460d2d388fa3389739de4073cc2.jpg)



Figure 1: Average performance on agentic, reasoning, and coding (ARC) benchmarks. Overall,GLM-4.5 achieves a rank of 3rd, with GLM-4.5-Air following at rank 6th. The models listed areevaluated as of July 28, 2025.


# 1 Introduction

Large language models (LLMs) are rapidly evolving from general knowledge repositories [6; 37; 50;33; 23] into general problem-solvers. The ultimate ambition, often associated with Artificial GeneralIntelligence (AGI), is to create models with human-level cognitive capabilities across diverse domains.This requires a unified mastery of complex problem-solving, generalization, and self-improvement,moving beyond task-specific excellence.

As LLMs become more integrated into real-world scenarios, the key to enhancing actual productivityand solving complex professional tasks lies in developing specific core capabilities. We identifythree critical, interconnected capabilities as the measure of a truly generalist model: Agentic abilitiesfor interacting with external tools and the real world; complex Reasoning for solving multi-stepproblems in domains like mathematics and science; and advanced Coding skills for tackling real-world software engineering tasks. While state-of-the-art proprietary models like OpenAI’s o1/o3 [18]and Anthropic’s Claude Sonnet 4 have demonstrated groundbreaking performance in specific ARCdomains (e.g., mathematical reasoning or code fixing [20]), a single, powerful open-source modelthat excels across all three areas has remained elusive.

This paper introduces two new models: GLM-4.5 and GLM-4.5-Air, toward the goal of unifying allthe different capabilities. The new models outperform existing open-source LLM models [13; 34; 47]across the board, with significant gains in agentic, reasoning, and coding tasks. GLM-4.5 and GLM-4.5-Air both feature hybrid reasoning modes: thinking mode for complex reasoning and agentictasks, and non-thinking mode for instant responses. GLM-4.5 is our first MoE model, with 355Btotal parameters and 32B activated parameters. GLM-4.5 demonstrates strong performance on thefollowing ARC benchmarks:

• Agentic: GLM-4.5 scores $7 0 . 1 \%$ on TAU-Bench and $7 7 . 8 \%$ on BFCL v3 [26], on par withClaude Sonnet 4. For web browsing agents, GLM-4.5 scores $2 6 . 4 \%$ on BrowseComp [45], clearlyoutperforming Claude Opus 4 $( 1 8 . 8 \% )$ and close to o4-mini-high $( 2 8 . 3 \% )$ .

• Reasoning: GLM-4.5 demonstrates outstanding performance on a suite of challenging reasoningbenchmarks, achieving $9 1 . 0 \%$ on AIME 24, $7 9 . 1 \%$ on GPQA [30], $7 2 . 9 \%$ on LiveCodeBench(2407-2501) [19], and $1 4 . 4 \%$ on HLE (Humanity’s Last Exam) [28].

• Coding: GLM-4.5 scores $6 4 . 2 \%$ on SWE-bench Verified [20] and $3 7 . 5 \%$ on Terminal-Bench [35],outperforming GPT-4.1 and Gemini-2.5-pro, close to Claude Sonnet 4.

GLM-4.5-Air is a smaller MoE model with 106B parameters. It represents a significant leap amongmodels at the 100B scale, matching or exceeding Qwen3-235B-A22B [47] and MiniMax-M1 [7].

In Figure 1, we show the average performance on 12 benchmarks across agentic, reasoning, andcoding (ARC) tasks. Overall, GLM-4.5 is ranked in the 3rd place and GLM-4.5-Air is ranked inthe 6th. On agentic tasks, GLM-4.5 is ranked in the 2nd place, following OpenAI o3. On codingtasks, GLM-4.5 is ranked in the third place, close to Claude Sonnet 4. Note that GLM-4.5 is highlyparameter-efficient, with only half the parameters of DeepSeek-R1 [13] and one-third those of KimiK2 [34]. In Figure 2, we report the scores on SWE-bench Verified vs model parameters of differentopen-source models, where GLM-4.5 and GLM-4.5-Air lie on the Pareto Frontier. More evaluationresults are detailed in Section 4.

Both GLM-4.5 and GLM-4.5-Air are available on Z.ai, BigModel.cn, and also as open-sourcemodels on https://huggingface.co/zai-org/GLM-4.5. We also open-sourced an evaluationtoolkit at https://github.com/zai-org/glm-simple-evals to ensure the reproducibility ofour benchmark results.

# 2 Pre-Training

# 2.1 Architecture

In the GLM-4.5 series, we adopt the MoE architecture, which improves the computational efficiencyof both training and inference. We employ loss-free balance routing [40] and sigmoid gates forMoE layers [23]. Different from DeepSeek-V3 [23] and Kimi K2 [34], we reduce the width (hiddendimension and number of routed experts) of the model and increase its height (number of layers), aswe found that deeper models exhibited better reasoning capacity. In the self-attention component,

![](images/98c30771bd7fec625be75c71e21bcf3adca32322d1da7587f1e35d3d508a10d5.jpg)



Figure 2: SWE-bench verified scores vs model parameters. Proprietary models are listed as unknownat the right side.


we employ Grouped-Query Attention with partial RoPE. Furthermore, we utilize 2.5 times moreattention heads (96 heads for a 5120 hidden dimension). Counterintuitively, while this increased headcount does not improve training loss compared to models with fewer heads, it consistently improvesperformance on reasoning benchmarks such as MMLU and BBH. We also incorporate QK-Norm [15]to stabilize the range of attention logits. For both GLM-4.5 and GLM-4.5-Air, we add an MoE layeras the MTP (Multi-Token Prediction) layer [12] to support speculative decoding during inference.


Table 1: Model architecture of GLM-4.5 and GLM-4.5-Air. When counting parameters, for GLM-4.5and GLM-4.5-Air, we include the parameters of MTP layers but not word embeddings and the outputlayer.


<table><tr><td>Model</td><td>GLM-4.5</td><td>GLM-4.5-Air</td><td>DeepSeek-V3</td><td>Kimi K2</td></tr><tr><td># Total Parameters</td><td>355B</td><td>106B</td><td>671B</td><td>1043B</td></tr><tr><td># Activated Parameters</td><td>32B</td><td>12B</td><td>37B</td><td>32B</td></tr><tr><td># Dense Layers</td><td>3</td><td>1</td><td>3</td><td>1</td></tr><tr><td># MoE Layers</td><td>89</td><td>45</td><td>58</td><td>60</td></tr><tr><td># MTP Layers</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>Hidden Dim</td><td>5120</td><td>4096</td><td>7168</td><td>7168</td></tr><tr><td>Dense Intermediate Dim</td><td>12288</td><td>10944</td><td>18432</td><td>18432</td></tr><tr><td>MoE Intermediate Dim</td><td>1536</td><td>1408</td><td>2048</td><td>2048</td></tr><tr><td>Attention Head Dim</td><td>128</td><td>128</td><td>192</td><td>192</td></tr><tr><td># Attention Heads</td><td>96</td><td>96</td><td>128</td><td>64</td></tr><tr><td># Key-Value Heads</td><td>8</td><td>8</td><td>128</td><td>64</td></tr><tr><td># Experts (total)</td><td>160</td><td>128</td><td>256</td><td>384</td></tr><tr><td># Experts Active Per Token</td><td>8</td><td>8</td><td>8</td><td>8</td></tr><tr><td># Shared Experts</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>QK-Norm</td><td>Yes</td><td>No</td><td>No</td><td>No</td></tr></table>

# 2.2 Pre-Training Data

Our pre-training corpus includes documents from webpages, social media, books, papers, and coderepositories. We carefully design the data processing pipelines for different sources.

Web The majority of our pre-training documents are English and Chinese webpages crawledfrom the Internet. Inspired by Nemotron-CC [32], we divide the crawled webpages into bucketsof different quality scores. We up-sample documents from the bucket with higher quality scores

![](images/22835e0b47f472b86da93cff19214bdf5aea09e8bb1c2a7a75ce50513ff18f21.jpg)



Figure 3: Pre-training and mid-training stages for GLM-4.5. We adapt a multi-stage training recipeand extend the sequence length from 4K to 128K.


and discard documents from the bucket with the lowest quality scores. The bucket with the highestquality scores contributes over 3.2 epochs during pre-training. In this way, the pre-training corpuscan emphasize the high-frequency knowledge for reasoning tasks and also improve coverage forlong-tail world knowledge. We have also found a large number of similar webpages automaticallygenerated from templates and assigned high scores. Such webpages cannot be removed by MinHashdeduplication. We additionally apply the SemDedup [1] pipeline to remove those similar webpagesbased on document embeddings.

Multilingual To support more natural languages, we include multilingual documents in our pre-training corpus. The multilingual corpus comes from both our crawled webpages and Fineweb-2 [27].We apply a quality classifier that judges the educational utility of documents and up-sample high-quality multilingual documents.

Code We curated source code data from GitHub and various code hosting platforms. The codecorpus undergoes a preliminary rule-based filtering, followed by classification using language-specificquality models that categorize samples into three tiers: high-quality, medium-quality, and low-quality.During training, we up-sampled high-quality code while excluding low-quality samples. Moreover,the Fill-In-the-Middle [5] training objective is applied to all source code data. For code-related webdocuments, we employ a two-stage retrieval process from our text pre-training corpus. Documentsare initially selected based on two criteria: presence of HTML code tags, or identification by aFastText [22] classifier trained to detect code-related content. Subsequently, the retrieved documentsundergo quality assessment using a dedicated model that classifies them into high-, medium-, orlow-quality categories, following the same quality-based sampling strategy for source code. Finally,a fine-grained parser is employed to re-parse the selected web pages to better preserve the formatsand contents of the code.

Math & Science To enhance the reasoning capacity, we collect documents related to mathematicsand science from webpages, books, and papers. We apply a large language model to score candidatedocuments based on the ratio of educational content about mathematics and science, and train asmall-scale classifier to predict the scores. Documents in the pre-training corpus with scores above acertain threshold are up-sampled.

The pre-training process of GLM-4.5 is divided into two stages. In the first stage, the model is mainlytrained on general documents from webpages. During the second stage, we up-sample the sourcecode from GitHub and webpages related to coding, mathematics, and science.

# 2.3 Mid-Training: Boost Reasoning & Agentic Capacity

After pre-training, we add several stages to further boost the model’s performance on importantapplication areas. Unlike traditional pre-training on large-scale general documents, these trainingstages utilize medium-size domain-specific datasets, including instruction data. Therefore, we denotethese training stages as mid-training, which includes the following.

Repo-level Code Training At this training stage, we add concatenated code files from the samerepository to learn cross-file dependency. To improve the model’s software engineering capability,

we also include model-filtered issues, pull requests (PRs), and commits from GitHub, with relatedissues, PRs, and commits concatenated into one context and commits organized in a diff-like format.We extend the training sequence length from 4K to 32K to incorporate large repositories.

Synthetic Reasoning Data Training At this stage, we add synthetic reasoning content for math,science, and coding competitions. We collect a large number of questions and answers related tothe reasoning tasks from webpages and books, and synthesize reasoning processes with a reasoningmodel.

Long-context & Agent Training To further boost the model’s long-context performance, weextend the training sequence length from 32K to 128K and up-sample long documents from thepre-training corpus. Large-scale synthetic agent trajectories are also incorporated at this stage.

In Figure 3, we show the complete stages for pre-training and mid-training. The maximum sequencelength is kept at 4,096 in pre-training, and is extended from 32,768 to 131,072 in mid-training. Duringpre-training, we did not use best-fit packing [11] since random truncation is a good data-augmentationstrategy for pre-training documents. For datasets in mid-training, we applied best-fit packing to avoidtruncating the reasoning process or repo-level code.

# 2.4 Hyper-Parameters

We employed the Muon optimizer [21; 24] for all parameters except word embedding, bias, andweights for RMSNorm. For hyperparameters, we set the Newton-Schulz iteration steps $N$ to 5,momentum $\mu$ to 0.95, and scaled Muon’s update RMS to 0.2. We observed that the Muon optimizercan accelerate convergence and tolerate larger batch sizes. We used cosine decay schedule for learningrate, instead of warmup-stable-decay (WSD) schedule [17]. Our early experiments showed thatmodels trained with the WSD schedule perform worse on general benchmarks (SimpleQA, MMLU),indicating underfitting in the stable stage. The learning rate went through a warm-up stage from 0 to2.5e-4 and a decaying stage to 2.5e-5 until the end of mid-training. We used a batch size warmupstrategy, where the batch size was gradually increased from 16M tokens to 64M tokens in the trainingof the first 500B tokens, and remained constant in the remaining of training. For regularization, weset the weight decay ratio to 0.1 and did not use dropout. We set the maximum sequence lengthto 4,096 during pre-training, and extended it to 32,768 and 131,072 during the mid-training stageas shown in Figure 3. When extending the sequence length to 32K, we also adjusted RoPE’s basefrequency from 10,000 to 1,000,000 for better long-context modeling ability. For loss-free balancerouting, we set the bias update rate to 0.001 for the first 15T tokens, and to 0.0 for the remainingtokens. We also applied auxiliary sequence-level balance loss with a 0.0001 weight to avoid extremeimbalance within any single sequence. The MTP loss weight $\lambda$ was set to 0.3 for the first 15T tokens,and to 0.1 for the remaining tokens.

# 3 Post-Training: Expert Model Iteration

We divide the post-training process into two distinct stages. In stage 1 (Expert Training), we constructexpert models specializing in three domains: Reasoning, Agent, and General chat. In stage 2 (UnifiedTraining), we employ self-distillation techniques to integrate multiple experts, ultimately delivering acomprehensive model capable of generating responses through both deliberative reasoning and directresponse modes.

# 3.1 Supervised Fine-Tuning

We perform Supervised Fine-Tuning (SFT) at the beginning of both Stage 1 (Expert Training) andStage 2 (Unified Training). In the expert training stage, the primary role of SFT is to provide a coldstart, empowering the model with basic chat, reasoning, and tool-use capabilities, which can then befurther enhanced in subsequent expert RL training to achieve improved performance. In the unifiedtraining stage, the purpose of SFT is to distill the capabilities of different expert models into onehybrid reasoning generalist capable of handling different types of tasks.

Cold Start SFT During the cold-start phase, we utilize a small set of supervised fine-tuning (SFT)data with extended Chain-of-Thought (CoT) responses. This approach ensures that each expert modelpossesses adequate foundational ability prior to the reinforcement learning phase.

Overall SFT In the Overall SFT stage, we collect millions of samples covering reasoning tasks(math, code, science, etc.), general chat (writing, translation, summarization, chit chat, etc.), agentictasks (basic tool using, coding ability especially for authentic project development, etc.), and long-context understanding tasks from the previously trained expert models, and train the base modelwith a maximum context length of 128K tokens. By distilling from the output of distinct experts,the model learns to apply the most effective long CoT reasoning for each task to arrive at accurateanswers. Especially, recognizing that a prolonged thinking process is unnecessary for certain domainsthat demand quick responses (such as chit chat), we meticulously balanced training data containingfull reasoning with data lacking explicit thought processes. This approach allows the model to operatein both the reflective and immediate response modes, thereby creating a hybrid reasoning model.Moreover, we find the following strategy helpful in preparing SFT data to derive optimal performance.

Reducing Character Escaping in Function Call Templates Although function call parameters arepredominantly represented in JSON format in contemporary implementations, a significant challengeemerges when these parameters contain code segments. In such cases, a substantial proportion ofcharacters within the code require escaping, compelling the model to generate extensive escapecharacters, thereby increasing the learning burden for the model. While this issue poses minimalconcern for models primarily designed for general chat, it represents a non-trivial challenge foragentic foundation models where function calling is a core capability. To mitigate this limitation,we propose a novel function call template that encapsulates function call keys and values withinXML-like special token tags. This approach substantially reduces the necessity for character escapingin code segments, as the vast majority of code can be represented in its native form without escaping.Experimental results demonstrate that the proposed function call template does not compromise theperformance of function call execution while recucing escaping. The following example (Figure 4)illustrates the structure of our proposed function call template. Detailed code implementation can befound in our open-source repository.

```txt
<|system|>   
# Tools   
You may call one or more functions to assist with the user query.   
You are provided with function signatures within <tools></tools> XML tags:   
{name": "get_weather", "description": "Get the weather of a city for a specific date.", "parameters": {" type": "object", "properties": {"city": {"type": "string", "description": "The city to get weather for, in Chinese.", "date": {"type": "string", "description": "The date in YYYY-MM-DD format.",}}, "required": ["city"]}}   
</tools>   
For each function call, output the function name and arguments within the following XML format: <tool_call>{function-name}   
{arg_key}{arg-key-1}</arg_key>   
{arg_value}{arg-value-1}</arg_value>   
{arg_key}{arg-key-2}</arg_key>   
{arg_value}{arg-value-2}</arg_value>   
...   
</tool_call></|system|>   
You are a helpful assistant.<|user|>   
Today is June 26, 2024. Could you please check the weather in Beijing and Shanghai for tomorrow<| assistant|>   
<think>The user wants to check the weather of Beijing and Shanghai tomorrow. I need to call the getweather function respectively to check Beijing and Shanghai.</think>   
I will call the getweather function to check the weather in Beijing and Shanghai.   
<tool_call>get/weather   
{arg_key}city</arg_key>   
{arg_value}Beijing</arg_value>   
{arg_key}date</arg_key>   
{arg_value}2024-06-27</arg_value>   
</tool_call>   
{tool_call}getweather   
{arg_key}city</arg_key>   
{arg_value}Shanghai</arg_value>   
{arg_key}date</arg_key>
```

```txt
<arg_value>2024-06-27</arg_value> </tool_call></observation|> <tool_response> {"city": "Beijing", "date": "2024-06-27", "weather": "Sunny", "temperature": "26C"} </tool_response> <tool_response> {"city": "Shanghai", "date": "2024-06-27", "weather": "Overcast", "temperature": "29C"} </tool_response></assistant> <think>I have obtained the weather query results of get_weather for Beijing and Shanghai respectively and can reply to users directly.</think> It will be sunny in Beijing tomorrow with a temperature of 26 degrees Celsius. The weather in Shanghai is overcast with a temperature of 29 degrees Celsius.</user|>
```

Figure 4: One example of function call template.

Rejection Sampling When sampling from expert models, we employ a comprehensive multistagefiltering pipeline that includes: (1) removing repetitive, excessively short, or truncated samples, aswell as those that fail to conform to valid reasoning formats; (2) conducting correctness verificationfor samples with objective answers; (3) utilizing reward models to filter responses to subjectivequestions; and (4) for tool-calling scenarios, ensuring adherence to proper tool invocation protocolsand verification that trajectories reach the expected terminal states.

Prompt Selection and Response-Level Scaling Filtering challenging prompts and conductingresponse scaling on them prove to be effective. We experimented with removing the prompts in thebottom $50 \%$ based on response lengths, resulting in a $2 \% - 4 \%$ improvement in math and sciencetasks, despite training with only half the data. Notably, we found that applying response scaling tothese hard prompts can lead to further gains. Generating four responses for each prompt brought anadditional $1 \% - 2 \%$ improvement.

Automatic Agentic SFT Data Construction The construction of agentic SFT data involves foursteps: 1. Agentic Framework and Tool Collection: We gather a set of agentic frameworks andreal-world tool APIs and MCP servers, while also leveraging LLMs to automatically construct andsimulate a batch of tools. 2. Task Synthesis: Based on these frameworks and tools, we automaticallysynthesize a collection of agentic tasks. On the one hand, for relatively mature frameworks, weleverage LLMs to comprehend their functionalities and automatically generate relevant queries ortasks. On the other hand, for more fragmented or disparate tools, we first select a representativesubset and similarly employ LLMs to construct tasks about this subset. These tasks encompass bothsingle-step and multi-step tool calling scenarios. 3. Trajectory Generation: For each synthesized task,we utilize existing LLMs to generate tool-call trajectories. Additionally, by employing the LLM as auser simulator, multi-step tool-call tasks are converted into trajectories involving multiple rounds ofdialogue. 4. Quality Filtering: For each trajectory, multiple judge agents are used to evaluate whetherthe task is completed. Only successful trajectories are retained.

# 3.2 Reasoning RL

Reasoning RL focuses on enhancing a model’s capabilities in domains that demand logical deduction,structured problem-solving, and verifiable accuracy. This includes critical areas such as mathematics,code generation, and scientific reasoning. A defining characteristic of these tasks is the high precisionof their reward signals, as correctness can often be determined programmatically or with objectiveclarity. Mastery in these areas is not only crucial for advancing the raw intelligence of modelsbut also serves as a fundamental building block for more complex, multi-step agentic behaviors.Recognizing the unique challenges and opportunities within reasoning RL, we have developed asuite of specialized techniques to effectively train our models. These methods, detailed below, aredesigned to address issues such as training efficiency, sample diversity, and data quality. Our overallRL algorithm builds upon the GRPO [31] framework, excluding the KL loss term. The comparisoncurves shown in this section are based on our smaller experimental model, not on GLM-4.5.

Difficulty-based Curriculum Learning During reinforcement learning, the model’s proficiencyevolves, creating a mismatch with static training data. In the later stages, as the model becomesmore capable, overly simple data can lead to rollouts where all rewards are 1s. Conversely, in theearly stages, excessively difficult data often results in batches where all rewards are 0s. In both


AIME24 AVG@32: RL with Difficulty-based Curriculum Learning


![](images/a7b0eea6a6e6a7bca16a082db09df53c4ed25d779fd30d3b5aa7c5103c02e287.jpg)



Figure 5: Effectiveness of the two-stage difficulty-based curriculum on AIME’24. The blue line(our method) switches to extremely difficult problems (pass $@ 8 { = } 0$ , pass $\textcircled { a } 5 1 2 \mathrm { > } 0$ ) in the secondstage, showing continued improvement. The red line (baseline) continues with moderate-difficultyproblems and plateaus.


![](images/97b2dbfbcedd19f446a19b145196689d78eefb0db7cd2251d9504c10f4b86746.jpg)



Figure 6: Single-stage vs. multi-stage RL at 64K context length. The red line (single-stage at64K) achieves superior performance. The blue line (multi-stage with progressively increasing length)suffers from an irreversible performance drop in early stages, limiting its final performance.


scenarios, the lack of reward variance provides no useful gradient signal, severely hindering trainingefficiency. To address this challenge, we employ a two-stage difficulty-based curriculum for RL. Theeffectiveness of this and other strategies discussed below is validated through controlled experimentson a smaller model, which allows for rapid iteration and precise ablation studies. As shown inFigure 5, this two-stage approach enables the model to consistently surpass its performance ceiling.Crucially, to maintain high signal quality and reduce noise, all problems used in the second stage arestrictly sourced from a pool with verified correct answers.

Single-Stage RL at 64K Output Length Previous research [25] has suggested conducting RL inmultiple stages with progressively increasing maximum output lengths. However, our experimentsreveal that this multi-stage approach is less effective than a single-stage RL process conducted directlyat the maximum target length of 64K. Since the initial Supervised Fine-Tuning (SFT) has alreadyconditioned the model on generating 64K-length responses, introducing RL stages with shortermaximum lengths can cause the model to “unlearn” its long-context capabilities. This often leads toa significant and irreversible drop in performance, as the model’s average output length decreases.This degradation is difficult to recover from in the final 64K-length RL stage, thus limiting furtherimprovement. Our experiments confirm this observation: as demonstrated in Figure 6, applying RLdirectly at the full 64K-length continually pushes the model’s limits and yields better performance.

![](images/60c5cff3a783c8f7364fbd868e141d8f291539c97368f53f46e528c7aeeab14a.jpg)


![](images/52a4567e5dfad242623f1b743a71a800124178bd3ae370bdd2bbaa51eb9677dd.jpg)



Figure 7: Ablation Studies for Code and Science RL. (Left) Comparison of loss calculationmethods for code RL. The token-weighted mean loss approach achieves faster convergence comparedto the sequence-mean loss baseline, accelerating the training process. (Right) Ablation on datasources for science RL on the GPQA-Diamond benchmark. Training exclusively on a small set ofhigh-quality, expert-verified multiple-choice questions yields the best performance, significantlyoutperforming training on mixed-quality data.


Dynamic Sampling Temperature During RL, the sampling temperature is a key parameter forcontrolling trajectory diversity. A temperature that is too low leads to convergent, less exploratoryoutputs, while one that is too high introduces low-quality, noisy samples, undermining both modelaccuracy and training efficiency. Using a fixed sampling temperature is suboptimal because it fails toadapt as the policy distribution becomes more concentrated (i.e., has lower entropy), often resultingin insufficient exploration at later stages. Therefore, we propose dynamically adjusting the samplingtemperature to maintain a healthy balance between accuracy and exploration. Specifically, whenthe average reward of rollouts stabilizes, we identify this as a convergence phase and increase thesampling temperature to encourage greater diversity. To mitigate the risk of introducing excessivenoise, we implement a quality-control mechanism: we periodically evaluate model performance on aheld-out validation set across a range of temperatures. The temperature for the next training phase isthen set to the maximum value that does not cause a performance drop of more than $1 \%$ from thecurrent optimum [2].

Code and Science RL Compared to mathematics, RL for coding and scientific domains hasreceived less attention in the literature. We conducted extensive controlled RL experiments in theseareas and arrived at the following empirical conclusions. For code RL, we find that the choiceof loss calculation is critical for training efficiency. As illustrated in Figure 7 (left), adopting atoken-weighted mean loss is highly beneficial compared to a conventional sequence-mean loss. Thetoken-weighted approach provides a finer-grained and more stable gradient signal, which leads tosignificantly faster convergence. This method also helps to alleviate the length bias inherent insequence-level rewards and effectively suppresses the generation of overly simplistic or repetitive“base case” samples during training. For science RL, our findings on the GPQA-Diamond benchmarkhighlight that data quality and type are paramount factors. As shown in Figure 7 (right), usingexclusively expert-verified multiple-choice questions for RL leads to significantly better performancecompared to training with mixed-quality or unverified data. This result underscores that even fortasks with simple formats like multiple-choice, rigorously filtering the RL data pool to include onlyhigh-quality, challenging instances is crucial for effective model improvement.

# 3.3 Agentic RL

Reinforcement Learning from Human Feedback (RLHF) helps language models follow humaninstructions more faithfully. Applying RL to math and programming contests has further uncoveredstrong reasoning abilities and favorable scaling behavior on tasks whose outcomes can be objectivelyverified. Building on these insights, we focus on agentic settings—specifically web-search andcode-generation agents—where every action or answer can be automatically checked. This built-inverifiability supplies dense, reliable rewards, enabling us to scale RL training more effectively.

# 3.3.1 Data Collection and Synthesis for Agents

For web-search tasks and open-domain information seeking, we develop a data-synthesis pipelinethat yields demanding question–answer pairs requiring multi-step reasoning across multiple websources. This corpus is designed to sharpen GLM’s ability to uncover elusive, interwoven facts onthe internet. Dataset construction blends two approaches: (1) an automated pipeline powered bymulti-hop reasoning over knowledge graphs, and (2) human-in-the-loop extraction and selectiveobfuscation of content from several web pages to prepare reinforcement-learning training signals.

For software-engineering tasks, we curate an extensive collection of GitHub pull requests and issuesto create a realistic software-development benchmark comprising user prompts and executable unittests. All evaluations run inside a hardened sandbox with a distributed system, which provides bothhorizontal scalability and strong isolation guarantees.

# 3.3.2 Pushing the Limits with Reinforcement Learning and Iterative Self-distillation

We adopt the group-wise policy optimization algorithm for RL training. For each problem $x$ , wesample $K$ agent traces $\{ y _ { 1 } , \ldots , y _ { k } \}$ from the previous policy $\pi _ { \mathrm { o l d } }$ , and optimize the model $\pi _ { \theta }$ withrespect to the following objective:

$$
L _ {\mathrm {R L}} (\theta) = \mathbb {E} _ {x \sim \mathcal {D}} \left[ \frac {1}{K} \sum_ {i = 1} ^ {K} (r (x, y _ {i}) - \bar {r} (x)) \right],
$$

where $\begin{array} { r } { \bar { r } ( x ) \ = \ \frac { 1 } { k } \sum _ { i = 1 } ^ { k } r \left( x , y _ { i } \right) } \end{array}$ is the mean reward of the sampled responses. It is noted that onlymodel-generated tokens are used for optimization, and the environment feedback is ignored in losscomputation.

Outcome Supervision with Process Action Format Penalty For web search tasks, we use theaccuracy of the final answer as a reward for the entire agent trace. For coding agents, we primarilyutilize SWE data with verifiable test cases for RL training. Our experiments have shown that RLtraining on web search and SWE tasks leads to generalized performance improvements across othertasks and benchmarks, such as general tool usage and coding tasks like Terminal-Bench. Additionally,we apply a process format penalty to ensure the model generates correct tool call formats. If themodel fails to produce the correct tool format during agent trace generation, the process will behalted, and the trace will receive a zero reward.

Iterative Distillation Since RL training on agent tasks is time-consuming, we adopt a self-distillation approach to iteratively enhance the performance of the SFT cold-start model beforeresuming RL training on this improved model. Specifically, we first perform RL training on theinitial cold-start model to boost agent performance. Once training has reached a certain step countor plateaued, we apply self-distillation by substituting the original cold-start data with responsesgenerated by the RL-trained model, thus creating a superior SFT model. We then conduct further RLtraining on this enhanced model, progressively increasing training difficulty. This iterative strategyallows us to push the performance limits of RL-trained models efficiently.

Scaling Test-time Compute via Interaction Turns For agent tasks, we observe significant perfor-mance gains given increasing interaction turns with the environment. Compared to test-time scalingin reasoning models, which scales output tokens, agent tasks make use of test-time compute bycontinuously interacting with the environment, e.g., searching high and low for hard-to-find webinformation or writing test cases for self-verification and self-correction for coding tasks. Figure 8shows that with varying browsing effort, accuracy scales smoothly with test-time compute.

# 3.4 General RL

General RL aims to holistically improve the model’s overall performance, remediate potential issues,and strengthen key capabilities. Central to our methodology is a multi-source feedback systemthat synergizes rule-based feedback, human feedback (RLHF), and model-based feedback (RLAIF).This hybrid framework provides more robust training signals and allows us to leverage the uniqueadvantages of each source: the precision of automated rules, the nuanced judgment of humanannotators, and the scalability of AI-driven evaluation.

![](images/bb77d14f65ff3577b620d1458d21c89b3c1a280056712b5aac04b2122d9840eb.jpg)



Figure 8: Interaction Turns Scaling for BrowseComp.


![](images/bb22b68d93bc3242f161ea75771390c15843a46bedefcbbd623704645f62f6e1.jpg)



Figure 9: Training curve of Instruction Following RL without other General RL tasks. DuringGRPO training, the instruction following performance (SysBench-ISR) improves in step with theincreasing reward. Up to roughly 1,000 training steps, we have not observed clear evidence of rewardhacking.


Holistic RL Holistic RL targets broad performance gains across diverse domains. To this end, wefirst construct a balanced dataset of roughly 5,000 prompts spanning 7 primary, 33 secondary, and139 tertiary categories. Reward signals for Holistic RL are derived from both human and AI feedback.For human feedback, we train a reward model on preference annotations. Annotators compare modelresponses and assign preference labels based on a comprehensive evaluation of multiple dimensions,such as instruction following, safety, and factual correctness. For model feedback, we design separatescoring rubrics that depend on whether the prompt has an objective ground-truth answer. Mergingthe two feedback sources yields more reliable and expressive reward signals, mitigating the inherentlimitations of each individual method.

Instruction Following RL Instruction Following RL improves the model’s ability to understandand satisfy complex instructions. To achieve this, we create a fine-grained taxonomy with 7 major and151 minor constraint types, covering content requirements, formatting rules, and more. Based on thistaxonomy, a dedicated training set of challenging instructions is assembled to cover every constrainttype. The feedback system consists of deterministic verification rules, a trained reward model, and acritique model. The robustness of this hybrid feedback system proves crucial during GRPO training.We observe mitigated reward hacking, enabling the policy model to achieve continuous and steadyimprovements in instruction following as shown in Figure 9.

Function Calling RL Function Calling RL is divided into step-wise rule-based RL and end-to-endmulti-turn RL. We incorporate step-wise rule-based RL directly into our general RL framework due

to their similar output lengths and convergence speeds. For end-to-end multi-turn RL, we first trainspecialized expert models and then distill these experts into the main model.

• Step-wise Rule-based RL: For tasks with clear tool invocation procedures, we annotate the groundtruth function call for each step/turn in the training data. Given the task and the function calls fromprevious steps/turns, the model is trained to generate the next assistant response, which can be afunction call or a response to the user. Using rule-based rewards, we guide the model to makecorrect function calls over consecutive rounds. Accordingly, we design the following strict rewardfunction:

$$
\text {R e w a r d} = \left\{ \begin{array}{l l} 1, & \text {i f F o r m a t C o r r e c t} (a _ {t}) \text {a n d M a t c h} (a _ {t}, a _ {t} ^ {*}) \\ 0, & \text {o t h e r w i s e} \end{array} \right.
$$

Here, $a _ { t }$ denotes the t-th function call generated by the model, and $a _ { t } ^ { * }$ is the corresponding groundtruth function call. A reward of 1 is only given if $a _ { t }$ is in the correct format and matches theground truth exactly (including the name, parameters, and every field). Otherwise, the reward is0. Such a strict reward rule not only guides the model to generate correct function calls but alsostrongly enforces output formatting, improving the model’s usability and robustness in real-worldinteractions.

• End-to-end Multi-turn RL: Step-wise rule-based RL decomposes tasks into static, predetermineddecision flows. In this process, the model lacks dynamic interactions with the environment andcannot autonomously explore, plan, or handle complex situations, thereby making its real-worldproblem-solving ability limited. To address these issues, we introduce end-to-end multi-turnfunction calling RL, where the model first generates the complete trajectory and is then rewardedbased on task completion. In this way, the model can optimize its action policy through continuoustrial and error with tool feedback, significantly enhancing its ability in autonomous planning anddecision-making. Specifically, end-to-end multi-turn function calling RL considers two types ofcomplex tasks: 1. single-turn multi-step tasks: The model needs to make multi-step function callsand interact with the environment to complete such tasks. We use complex tasks automaticallysynthesized based on MCP servers, as well as some open-source agentic datasets with runnableenvironments, such as Agentgym [46]. 2. multi-turn multi-step tasks: Besides interacting with thetool execution environment, the model also needs to interact with an LLM-simulated user agentto obtain complete task information and accomplish the overall task. The reward for end-to-endmulti-turn function calling RL is computed as:

$$
\text {R e w a r d} = \left\{ \begin{array}{l l} 1, & \text {i f F o r m a t C o r r e c t} (a _ {1}, \ldots , a _ {T}) \text {a n d T a s k C o m p l e t e d} (I, o _ {0}, a _ {1}, o _ {1}, \ldots , a _ {T}, o _ {T}) \\ 0, & \text {o t h e r w i s e} \end{array} \right.
$$

Here, $I$ refers to the original complex task, $a _ { t }$ is the t-th function call, and $o _ { t }$ is the tool feedbackor user information. TaskCompleted $( I , o _ { 0 } , a _ { 1 } , o _ { 1 } , \dots , a _ { T } , o _ { T } )$ indicates whether the task iscompleted, which is determined by the environment according to predefined rules or by an LLMJudge Agent.

Pathology RL As the final stage of post-training, general RL needs to rectify potential issues,such as language mixing, excessive repetition, and formatting mistakes. Although penalizing suchbehaviors in the above-mentioned general RL tasks is effective, the low incidence rate of thesepathologies (often less than $1 \%$ of outputs) makes this a sample-inefficient optimization strategy.Therefore, we curate a targeted dataset for pathology RL by identifying prompts that are highly likelyto trigger these pathological behaviors. Training on this dataset lets us impose efficient penalties,further lowering the residual error rates for these problematic behaviors.

# 3.5 RL Infrastructure

Our RL infrastructure is built upon $\mathrm { S l i m e ^ { 1 } }$ , an open-source framework we developed. The frameworkis engineered with several key optimizations to enhance flexibility, efficiency, and extensibility.

Flexible Hybrid Training and Data Generation Architecture A core feature of our infrastructureis its support for highly flexible training paradigms and data generation strategies within a single,unified system. This design allows us to cater to the distinct requirements of various RL tasks by

![](images/97f8eb3ac7d99e718d653f126472cf0bc87471e636829ae92e1fb09971233008.jpg)



Figure 10: Overview of the Slime RL infrastructure. The system consists of three core modules:Training (Megatron) – handles the main training process, reads data from the Data Buffer, andsynchronizes parameters with the rollout module after training; Rollout (SGLang $^ +$ Router) –generates new data, including rewards and verifier outputs, and writes it to the Data Buffer; DataBuffer – serves as a bridge module that manages prompt initialization, custom data, and rolloutgeneration strategies.


supporting both a colocated, synchronous mode and a disaggregated, asynchronous mode. Thisflexibility in data generation is crucial for extending our RL capabilities to new domains and morecomplex agentic environments. We observed that different RL tasks benefit from different schedulingapproaches. For general-purpose RL tasks or those aimed at enhancing model reasoning capabilities(e.g., in mathematics and code generation), a synchronous, colocated architecture is more effective. Inthis setup, the training and inference engines reside on the same worker. This, combined with dynamicsampling, significantly reduces GPU idle time and maximizes resource utilization. Conversely, foragentic tasks, such as those in Software Engineering (SWE), the data generation process is oftenprotracted and involves complex system interactions. To ensure that the agent environments canoperate continuously and maximize data throughput, we adopt a disaggregated, asynchronous model.The rollout component of the RL framework is exposed directly to the agent environment, whilethe GPUs for training and inference are scheduled independently. This decoupling enables theagent environments to constantly generate new data without being stalled by the training cycle. Byleveraging the resource scheduling and asynchronous capabilities of the Ray framework, we canflexibly place the inference and training engines on the same GPU or on different ones. This dualsupport for synchronous and asynchronous training allows diverse RL tasks to share a common set ofunderlying optimizations for both training and inference.

Accelerated Rollout with Mixed-Precision Inference Rollout efficiency is a persistent bottleneckin RL training. To address this, our infrastructure supports BF16 for training while leveraging FP8for inference to accelerate the data generation phase. During each policy update iteration, we performonline, block-wise FP8 quantization on the model parameters before they are dispatched for rollout.This dynamic quantization enables highly efficient FP8 inference, significantly improving the overallthroughput of the data collection process.

Agent-oriented RL Infra Design To conduct RL for agent tasks, we design a fully asynchronousand decoupled RL infrastructure that efficiently handles long-horizon agent rollouts and supportsflexible multi-task RL training across diverse agent frameworks.

Agentic rollouts often require prolonged interactions with complex environments, which can signifi-cantly slow down the overall RL training process. To overcome this, we first design a high-concurrencyDocker-based runtime that provisions isolated environments for each task, drastically reducing rolloutoverhead. In addition, we implement a fully asynchronous RL training loop. Because agent tasks canvary in type and trajectory length, synchronous RL training often leads to severe GPU underutilizationas workers wait for the slowest rollouts to complete. Our approach partitions GPUs into dedicatedrollout engines and training engines: the rollout engines continuously generate trajectories, whilethe training engines update the model weights and periodically synchronize them back to the rolloutengines. This decoupled design prevents long or diverse trajectories from blocking the entire training

pipeline, resulting in consistently high throughput, particularly in scenarios with highly variable agentinteractions.

Another key challenge is the diversity of existing agent frameworks, which are tailored to differenttasks. Leveraging these frameworks not only improves task-specific performance but also maintainsalignment between training and inference. To achieve this, we introduce a unified HTTP endpointinterface coupled with a centralized data pool. Since most agent frameworks produce rollouts in amessage-list format, all trajectories are stored in this data pool, which serves as a shared source fortraining. This architecture cleanly decouples task-specific rollout logic from the RL training process,enabling seamless integration of heterogeneous agent frameworks. Furthermore, the data poolsupports customizable, task-specific filtering and dynamic sampling strategies to ensure high-qualityRL training data across diverse tasks.

Through these two core designs, our system provides a scalable, flexible, and high-performancesolution for long-agentic RL, and can support long-horizon rollouts and adapt to a wide range ofagent tasks.

# 4 Evaluation

# 4.1 Evaluation of Base Models

We first evaluate the performance of our base model GLM-4.5-Base. Table 2 shows the comparisonresults of the last checkpoint of pre-training of our base model. Please note that the base model hasnot been trained on instruction data, and the GLM-4.5-Base scores are from our internal evaluationframework. Results show that GLM-4.5-Base is stable on all the different benchmarks, includingEnglish, Code, Math, and Chinese, which validates our idea of unifying all the abilities into onemodel.


Table 2: Comparison among GLM-4.5-Base and other representative open-source base models.


<table><tr><td></td><td>Benchmark (Metric)</td><td>Qwen3-235B-A22B Base</td><td>Llama4-Maverick 400B Base</td><td>DeepSeek-V3 Base</td><td>Kimi-K2 Base</td><td>GLM-4.5 Base</td></tr><tr><td></td><td>Architecture</td><td>MoE</td><td>MoE</td><td>MoE</td><td>MoE</td><td>MoE</td></tr><tr><td></td><td># Activated Params</td><td>22B</td><td>17B</td><td>37B</td><td>32B</td><td>32B</td></tr><tr><td></td><td># Total Params</td><td>235B</td><td>400B</td><td>671B</td><td>1043B</td><td>355B</td></tr><tr><td rowspan="6">English</td><td>SimpleQA (EM)</td><td>-</td><td>-</td><td>26.6</td><td>35.3</td><td>30.0</td></tr><tr><td>BBH (EM)</td><td>88.9</td><td>87.1</td><td>88.4</td><td>88.7</td><td>86.2</td></tr><tr><td>MMLU (EM)</td><td>87.8</td><td>85.2</td><td>87.2</td><td>87.8</td><td>86.1</td></tr><tr><td>HellaSwag (EM)</td><td>-</td><td>-</td><td>88.9</td><td>94.6</td><td>87.1</td></tr><tr><td>PIQA (EM)</td><td>-</td><td>-</td><td>84.7</td><td>-</td><td>85.3</td></tr><tr><td>TriviaQA (EM)</td><td>-</td><td>-</td><td>82.9</td><td>85.1</td><td>80.0</td></tr><tr><td rowspan="2">Code</td><td>EvalPlus (Pass@1)</td><td>77.6</td><td>65.5</td><td>65.6</td><td>80.3</td><td>78.1</td></tr><tr><td>LiveCodeBench-Base (Pass@1)</td><td>-</td><td>25.1</td><td>24.6</td><td>26.3</td><td>28.1</td></tr><tr><td rowspan="2">Math</td><td>GSM8K (EM)</td><td>94.4</td><td>87.7</td><td>87.6</td><td>92.1</td><td>79.4</td></tr><tr><td>MATH (EM)</td><td>71.8</td><td>63.3</td><td>62.6</td><td>70.2</td><td>61.0</td></tr><tr><td rowspan="4">Chinese</td><td>CLUEWSC (EM)</td><td>-</td><td>-</td><td>82.7</td><td>-</td><td>83.5</td></tr><tr><td>C-Eval (EM)</td><td>-</td><td>80.9</td><td>90.1</td><td>92.5</td><td>86.9</td></tr><tr><td>C3 (EM)</td><td>-</td><td>-</td><td>78.6</td><td>-</td><td>83.1</td></tr><tr><td>Chinese-SimpleQA (EM)</td><td>-</td><td>53.5</td><td>72.1</td><td>77.6</td><td>70.1</td></tr></table>

# 4.2 Evaluation on 12 (ARC) Benchmarks

We further evaluate our full GLM-4.5 models after Post-Training for all the Agentic, Reasoning, andCoding (ARC) tasks, on 12 benchmarks: MMLU-Pro, AIME 24, MATH-500, SciCode, GPQA, HLE,LCB (2407-2501), SWE-Bench Verified, Terminal-Bench, TAU-Bench, BFCL V3, BrowseComp.

# 4.2.1 Evaluation of Agentic Abilities

We evaluate the agentic abilities of GLM-4.5 in two aspects: TAU-bench [48] (including retail andairline domains) and Berkeley Function Call Leaderboard V3 (BFCL V3) [26], which measures themodel’s ability to call user-defined functions to respond to users’ queries. BrowseComp [45] measuresthe model’s ability as a web browsing agent to find correct answers for complicated questions. For


Table 3: Results on Agentic Benchmarks. TAU represents TAU-bench [48] and BFCL representsBerkeley Function Calling Leaderboard [26].


<table><tr><td>Benchmark</td><td>GLM-4.5</td><td>GLM-4.5-Air</td><td>o3</td><td>o4mini</td><td>GPT-4.1</td><td>ClaudeOpus 4</td><td>ClaudeSonnet 4</td><td>Gemini2.5 Pro</td><td>KimiK2</td><td>Grok 4</td></tr><tr><td>TAU-Retail</td><td>79.7</td><td>77.9</td><td>70.4</td><td>65.6</td><td>75.1</td><td>81.4</td><td>80.5</td><td>77.0</td><td>73.9</td><td>76.5</td></tr><tr><td>TAU-Airline</td><td>60.4</td><td>60.8</td><td>52.0</td><td>49.2</td><td>48.8</td><td>59.6</td><td>60.0</td><td>48.0</td><td>51.2</td><td>58.4</td></tr><tr><td>BFCL V3</td><td>77.8</td><td>76.4</td><td>72.4</td><td>67.2</td><td>68.9</td><td>74.4</td><td>75.2</td><td>61.2</td><td>71.1</td><td>66.2</td></tr><tr><td>BrowseComp</td><td>26.4</td><td>21.3</td><td>49.7</td><td>28.3</td><td>4.1</td><td>18.8</td><td>14.7</td><td>7.6</td><td>7.9</td><td>32.6</td></tr><tr><td>Average</td><td>58.1</td><td>55.7</td><td>61.1</td><td>50.1</td><td>45.0</td><td>54.6</td><td>53.4</td><td>43.8</td><td>47.2</td><td>55.4</td></tr></table>

TAU-bench, we use an optimized user simulator (Cf. Figure 11) for both the Retail and Airlinedomains. The user prompt we use can be found below in Figure 11. On TAU-bench, GLM-4.5’sperformance is better than Gemini 2.5 Pro and close to Claude Sonnet 4. On BFCL V3, GLM-4.5achieves the best overall score among the baselines. On BrowseComp, the performance of OpenAIo3 is much better than that of other models. GLM-4.5’s performance is close to the second-bestmodel (o4-mini) and significantly better than Claude Opus 4.

You are a user interacting with an agent.{instruction_display}

# Rules:

- Just generate one line at a time to simulate the user’s message.

- Do not give away all the instruction at once. Only provide the information thatis necessary for the current step.

- Do not hallucinate information that is not provided in the instruction. Followthese guidelines:

1. If the agent asks for information NOT in the instruction:

- Say you don’t remember or don’t have it

- Offer alternative information that IS mentioned in the instruction

2. Examples:

- If asked for order ID (not in instruction): ‘‘Sorry, I don’t remember the orderID, can you search for it? My name/email/phone number/zipcode is ...’’

- If asked for email (not in instruction): ‘‘I don’t have my email handy, but Ican give you my name and zip code which are...’’

- Do not repeat the exact instruction in the conversation. Instead, use your ownwords to convey the same information.

- Try to make the conversation as natural as possible, and stick to thepersonalities in the instruction.

# Constraint Handling:

- Provide requests strictly based on what is explicitly stated in the instruction.

- Do not assume, extend, substitute, or generalize in any form.

- Do not modify or relax constraints on:

- Time / Date

- Budget

- Specific terms (e.g., ‘‘same’’ must not be replaced with ‘‘similar’’)

- Core Rule: Any attribute NOT mentioned in the instruction can be either changedor kept the same

- Examples:

- If instruction says ‘‘exchange red item to blue’’: Only color must change, otherattributes (size, material, etc.) are flexible

- If instruction says ‘‘exchange red item to blue, keep the same size’’: Bothcolor must change AND size must stay the same

- Exception: Only follow additional constraints when explicitly stated in theinstruction

# When NOT to finish the conversation:

- Do not end until you have clearly and completely expressed all your requirementsand constraints.

- Do not end until the agent has completed all tasks mentioned in the instructionand verified no operations were missed.

- Do not end if the agent’s execution results do not match your expectations orare incorrect/incomplete.

# When you CAN finish the conversation:

```txt
- Only when all above conditions are satisfied AND all tasks are completed correctly.
- OR when you have clearly expressed complete requirements but the system explicitly states it cannot complete them due to technical limitations - in this case, accept transfer to human.
# How to finish the conversation:
- If the agent has completed all tasks, generate ‘‘#####STOP##’’ as a standalone message without anything else to end the conversation.
# Note:
- You should carefully check if the agent has completed all tasks mentioned in the instruction before generating ‘‘####STOP##’’.
```

Figure 11: One example of user prompt we used for TAU-bench.

# 4.2.2 Evaluation of Reasoning

We evaluate the reasoning abilities of GLM-4.5 and GLM-4.5-Air on seven benchmarks, includingMMLU-Pro [43], AIME 24, MATH 500 [14], SciCode [36], GPQA [30], Humanity’s Last Exam(HLE) [28], and LiveCodeBench (LCB) $[ 1 9 ] ^ { 2 }$ . For the AIME and GPQA benchmarks, we report theaverage accuracy over 32 and 8 samples, respectively (Avg@32, Avg@8), to mitigate result variance.An LLM was used for automated answer validation. For the HLE benchmark, only the text-basedquestions were evaluated, with correctness judged by GPT-4o. Our evaluation code is also open-sourced3. We also compute the average reasoning performance on the seven benchmarks with theintelligence index proposed by Artificial Analysis4. GLM-4.5 outperforms OpenAI o3 on AIME 24and SciCode. On average, GLM-4.5 outperforms Claude Opus 4 and is close to DeepSeek-R1-0528.


Table 4: Results on Reasoning Benchmarks. HLE represents Humanity’s Last Exam [28] and LCBrepresents LiveCodeBench (2407-2501) [19].


<table><tr><td>Benchmark</td><td>GLM-4.5</td><td>GLM-4.5-Air</td><td>o3</td><td>Claude Opus 4</td><td>Gemini 2.5 Pro</td><td>DeepSeek R1 0528</td><td>Qwen3 235B 2507</td><td>Grok 4</td></tr><tr><td>MMLU Pro</td><td>84.6</td><td>81.4</td><td>85.3</td><td>87.3</td><td>86.2</td><td>84.9</td><td>84.5</td><td>86.6</td></tr><tr><td>AIME 24</td><td>91.0</td><td>89.4</td><td>90.3</td><td>75.7</td><td>88.7</td><td>89.3</td><td>94.1</td><td>94.3</td></tr><tr><td>MATH 500</td><td>98.2</td><td>98.1</td><td>99.2</td><td>98.2</td><td>96.7</td><td>98.3</td><td>98.0</td><td>99.0</td></tr><tr><td>SciCode</td><td>41.7</td><td>37.3</td><td>41.0</td><td>39.8</td><td>42.8</td><td>40.3</td><td>42.9</td><td>45.7</td></tr><tr><td>GPQA</td><td>79.1</td><td>75.0</td><td>82.7</td><td>79.6</td><td>84.4</td><td>81.3</td><td>81.1</td><td>87.7</td></tr><tr><td>HLE</td><td>14.4</td><td>10.6</td><td>20.0</td><td>11.7</td><td>21.1</td><td>14.9</td><td>15.8</td><td>23.9</td></tr><tr><td>LCB</td><td>72.9</td><td>70.7</td><td>78.4</td><td>63.6</td><td>80.1</td><td>77.0</td><td>78.2</td><td>81.9</td></tr><tr><td>AA-Index (Est.)</td><td>67.7</td><td>64.8</td><td>70.0</td><td>64.4</td><td>70.5</td><td>68.3</td><td>69.4</td><td>73.2</td></tr></table>

# 4.2.3 Evaluation of Coding


Table 5: Results on SWE-bench Verified and Terminal-Bench


<table><tr><td>Benchmark</td><td>GLM-4.5</td><td>GLM-4.5-Air</td><td>o3</td><td>GPT-4.1</td><td>Claude Opus 4</td><td>Claude Sonnet 4</td><td>Gemini 2.5 Pro</td><td>DeepSeek R1 0528</td><td>Kimi K2</td></tr><tr><td>SWE-bench Verified</td><td>64.2</td><td>57.6</td><td>69.1</td><td>48.6</td><td>67.8</td><td>70.4</td><td>49.0</td><td>41.4</td><td>65.4</td></tr><tr><td>Terminal-Bench</td><td>37.5</td><td>30.0</td><td>30.2</td><td>30.3</td><td>43.2</td><td>35.5</td><td>25.3</td><td>17.5</td><td>25.0</td></tr><tr><td>Average</td><td>50.9</td><td>43.8</td><td>49.7</td><td>39.5</td><td>55.5</td><td>53.0</td><td>37.2</td><td>29.5</td><td>45.2</td></tr></table>

To measure GLM-4.5’s ability to complete real-world coding tasks, we evaluate it on two challengingbenchmarks, SWE-bench Verified [20] and Terminal-Bench [35]. SWE-bench measures the model’sability to modify an existing codebase to solve a GitHub issue. The Verified subset is a human-filtered subset of 500 instances. For evaluation, we use OpenHands [42] v0.34.0 with runs limited to

100 iterations and history truncation to prevent exceeding the 128K context limit, configured withtemperature $_ { = 0 . 6 }$ , top $\mathtt { p } { = } 1 . 0$ . Terminal-Bench measures the model’s ability to accomplish complextasks in a terminal environment. We use the Terminus framework and standard function calling ratherthan direct prompting for evaluation. On SWE-bench Verified, GLM-4.5 outperforms GPT-4.1 andGemini-2.5-Pro. On Terminal-Bench, GLM-4.5 outperforms Claude Sonnet 4. On average, GLM-4.5is the best competitor for Claude Sonnet 4 on coding tasks.

# 4.2.4 Evaluation of General Abilities


Table 6: Results on Commonly Used General Chat Benchmarks


<table><tr><td>Benchmark</td><td>GLM-4.5</td><td>GLM-4.5-Air</td><td>GPT-4.1</td><td>Claude Sonnet 4</td><td>Gemini 2.5 Pro</td><td>Grok 4</td><td>Qwen3 235B</td><td>Deepseek R1 0528</td><td>DeepSeek V3 0324</td><td>Kimi K2</td></tr><tr><td>MMLU</td><td>90.0</td><td>87.4</td><td>90.2</td><td>91.9</td><td>91.9</td><td>91.9</td><td>90.2</td><td>89.9</td><td>89.1</td><td>89.5</td></tr><tr><td>SimpleQA</td><td>26.4</td><td>14.5</td><td>42.3</td><td>18.5</td><td>54.0</td><td>51.9</td><td>45.8</td><td>27.8</td><td>27.7</td><td>31.0</td></tr><tr><td>IFEval</td><td>86.1</td><td>86.3</td><td>87.4</td><td>88.7</td><td>90.8</td><td>92.4</td><td>87.8</td><td>80.0</td><td>83.4</td><td>89.8</td></tr><tr><td>SysBench</td><td>81.0</td><td>77.4</td><td>80.6</td><td>80.6</td><td>82.2</td><td>81.5</td><td>83.3</td><td>81.2</td><td>79.8</td><td>79.0</td></tr><tr><td>MultiChallenge</td><td>52.8</td><td>42.5</td><td>38.3</td><td>55.3</td><td>57.5</td><td>65.2</td><td>58.2</td><td>46.5</td><td>37.0</td><td>54.1</td></tr></table>

To evaluate the model’s general abilities, we employed a set of widely-adopted open-source bench-mark datasets, encompassing knowledge-intensive evaluations MMLU (EM) [14] and SimpleQA (Cor-rect) [44], and instruction-following assessments IFEval (Prompt Strict) [52], SysBench (ISR) [29],and MultiChallenge [10]. MultiChallenge is a multi-turn conversational benchmark evaluating LLMsacross four integrated capability dimensions. SysBench systematically evaluates LLMs’ systemmessage following capabilities across multi-turn conversations through three-level granularity metrics.On the MMLU benchmark, nearly all flagship models, including GLM-4.5, demonstrate performanceat a comparable level. SimpleQA, which reflects the factual knowledge of a model, shows thatGLM-4.5 (355B) performs similarly to DeepSeek V3 and R1 (both 671B), despite having nearlyhalf of the parameters. On the IFEval benchmark, GLM-4.5 outperforms DeepSeek R1. In theSysbench evaluation, GLM-4.5 surpasses GPT-4.1, DeepSeek V3, and Kimi K2. Additionally, on theMultiChallenge benchmark, it demonstrates superior performance compared to both GPT-4.1 andDeepSeek R1.

# 4.2.5 Evaluation of Safety

To systematically assess the safety alignment of our model, we utilized SafetyBench [51], a compre-hensive benchmark designed to evaluate the safety of large language models. SafetyBench consistsof 11,435 multiple-choice questions covering seven distinct categories of safety concerns, with datain both English and Chinese. This benchmark enables a standardized and scalable evaluation of amodel’s ability to handle potentially harmful or sensitive topics. The categories include Ethics andMorality, Illegal Activities, Mental Health, Offensiveness, Physical Health, Privacy and Property, andUnfairness and Bias. We evaluated GLM-4.5 against a suite of other leading models. The resultsindicate that GLM-4.5 achieves a strong safety score, competitive with other top-tier models. Itsoverall score of 89.87 is comparable to that of Kimi-K2 (90.48) and GPT-4.1 (89.71). Notably,GLM-4.5 demonstrates robust performance in the areas of Ethics and Morality (94.33), Mental Health(94.67), and Physical Health (96.67). While it performs well in preventing responses related to IllegalActivities (90.97) and protecting Privacy and Property (92.00), there is still room for improvement inaddressing Unfairness and Bias, an area of ongoing focus for our development efforts. The detailedperformance breakdown is presented in the table below.

# 4.3 Evaluations for Hands-on Experience

Sometimes, a trained LLM may overfit some predefined benchmarks, which makes the evaluatedresults not precisely reflect real-world experience. To overcome this challenge and to gauge ourmodel’s performance in more realistic situations, we have established a comprehensive manualevaluation framework. Human evaluation is particularly advantageous for assessing performanceon open-ended questions, where aspects like coherence, relevance, and creativity are paramount.This hands-on approach allows for a more granular analysis, enabling us to better pinpoint areas ofweakness and understand the qualitative aspects of model behavior that automated metrics often miss.


Table 7: Evaluation Results on SafetyBench


<table><tr><td>Model</td><td>Average</td><td>Ethics &amp; Morality</td><td>Illegal Activities</td><td>Mental Health</td><td>Offensiveness</td><td>Physical Health</td><td>Privacy &amp; Property</td><td>Unfairness &amp; Bias</td></tr><tr><td>GLM-4.5</td><td>89.9</td><td>94.3</td><td>91.0</td><td>94.7</td><td>83.0</td><td>96.7</td><td>92.0</td><td>77.4</td></tr><tr><td>GLM-4.5-Air</td><td>87.8</td><td>91.0</td><td>90.3</td><td>92.7</td><td>83.3</td><td>92.3</td><td>90.3</td><td>74.7</td></tr><tr><td>Gemini 2.5 Pro</td><td>90.5</td><td>94.7</td><td>91.7</td><td>95.3</td><td>84.3</td><td>97.0</td><td>92.3</td><td>78.0</td></tr><tr><td>Kimi K2</td><td>90.5</td><td>93.0</td><td>93.3</td><td>95.0</td><td>90.3</td><td>97.3</td><td>93.0</td><td>71.3</td></tr><tr><td>GPT-4.1</td><td>89.7</td><td>92.0</td><td>94.3</td><td>95.3</td><td>85.3</td><td>95.7</td><td>91.3</td><td>74.0</td></tr><tr><td>DeepSeek-V3-0324</td><td>88.8</td><td>92.3</td><td>90.7</td><td>95.0</td><td>84.7</td><td>95.7</td><td>91.0</td><td>72.3</td></tr><tr><td>DeepSeek-R1-0528</td><td>83.5</td><td>87.0</td><td>81.7</td><td>86.7</td><td>77.7</td><td>92.0</td><td>85.7</td><td>73.7</td></tr></table>

# 4.3.1 Evaluation of General Chat

To test the practical application capabilities of our models, we curated a diverse dataset of real-scenario user prompts. These prompts span multiple languages and cover a wide range of categories,including Mathematics, Text Processing, Text Generation, Subjective QA, Objective QA, LogicalReasoning, and Code Instructions. We meticulously filtered this collection to ensure high quality andappropriate difficulty, while also removing any data that could compromise user privacy or safety.The final dataset consists of 660 prompts, with a distribution of 392 in English, 108 in Chinese, and160 in other languages. For prompts requiring factual knowledge, we annotated the correct answersto serve as a ground truth for evaluation.

We conducted a comparative evaluation between GLM-4.5, Deepseek-R1-0528, and Kimi K2. Foreach prompt, the responses from the different models were presented in a randomized order toeliminate any potential sequential bias. A single, consistent evaluator then scored each responseon a scale of 0 to 10. This method of using the same evaluator at the same time for a batch ofcomparisons is designed to minimize deviations arising from different individual preferences andsubjective standards. The reasoning contents of GLM-4.5 and Deepseek-R1-0528 are not presentedto the evaluators. The average scores for each model across the different categories and languages arepresented below.

English Results In the English prompt set, GLM-4.5 achieved the highest overall score. It demon-strated particularly strong performance in Mathematics, Objective QA, and Text Generation.


Table 8: Human Evaluation Scores on English Prompts. Subj. stands for Subjective. Obj stands forObjective. Text Gen. stands for Text Generation.


<table><tr><td>Model</td><td>Overall</td><td>Math</td><td>Text Proc.</td><td>Subj. QA</td><td>Obj. QA</td><td>Text Gen.</td><td>Logic</td><td>Code</td></tr><tr><td>GLM-4.5</td><td>8.66</td><td>8.72</td><td>8.00</td><td>8.36</td><td>8.82</td><td>8.61</td><td>9.25</td><td>8.53</td></tr><tr><td>DeepSeek-R1-0528</td><td>8.62</td><td>8.56</td><td>8.27</td><td>7.91</td><td>9.00</td><td>7.83</td><td>9.07</td><td>8.65</td></tr><tr><td>Kimi-K2</td><td>8.13</td><td>7.22</td><td>8.00</td><td>7.45</td><td>8.86</td><td>7.06</td><td>7.07</td><td>8.71</td></tr></table>

Chinese Results For the Chinese prompts, GLM-4.5 again led with the highest average score,showing standout performance in Text Generation, Logical Reasoning, and Code Instructions.


Table 9: Manual Evaluation Scores on Chinese Prompts


<table><tr><td>Model</td><td>Overall</td><td>Math</td><td>Text Proc.</td><td>Subj. QA</td><td>Obj. QA</td><td>Text Gen.</td><td>Logic</td><td>Code</td></tr><tr><td>GLM-4.5</td><td>8.37</td><td>7.68</td><td>8.20</td><td>8.50</td><td>8.66</td><td>9.00</td><td>9.27</td><td>8.89</td></tr><tr><td>DeepSeek-R1-0528</td><td>8.05</td><td>7.76</td><td>8.07</td><td>8.00</td><td>7.89</td><td>8.59</td><td>9.00</td><td>8.67</td></tr><tr><td>Kimi-K2</td><td>7.03</td><td>7.37</td><td>6.43</td><td>7.71</td><td>6.45</td><td>8.28</td><td>7.55</td><td>8.26</td></tr></table>

Other Languages Results In the multilingual evaluation covering other languages, GLM-4.5maintained its lead, excelling in Text Generation and Subjective QA.


Table 10: Manual Evaluation Scores on Other Language Prompts


<table><tr><td>Model</td><td>Overall</td><td>Math</td><td>Text Proc.</td><td>Text Gen.</td><td>Subj. QA</td><td>Obj. QA</td><td>Code</td><td>Logic</td></tr><tr><td>GLM-4.5</td><td>8.49</td><td>8.67</td><td>8.13</td><td>8.90</td><td>9.33</td><td>8.71</td><td>7.86</td><td>8.33</td></tr><tr><td>DeepSeek-R1-0528</td><td>8.27</td><td>9.44</td><td>8.38</td><td>7.86</td><td>9.44</td><td>8.22</td><td>7.64</td><td>8.17</td></tr><tr><td>Kimi-K2</td><td>6.63</td><td>7.22</td><td>6.38</td><td>7.62</td><td>7.78</td><td>6.22</td><td>6.68</td><td>7.17</td></tr></table>

# 4.3.2 Evaluation of Coding Agent


GLM-4.5's Experience with Agentic Coding inReal-world Development Scenarios


![](images/5e64fe1b282cc56996e565db29e51d9048a83e4b7da7c3086d31736aea5cb513.jpg)


![](images/b492186d3f844a2668b970e824fc8e00dcc8a80cfc84623f3cf527e369a829c5.jpg)



Figure 12: Head-to-head evaluation results between GLM-4.5 and other models on CC-Bench.



Average Tool Calling Success Rate Comparison


![](images/55bdbe296211b0e0ca2265e1d001ae0024470075e8e7c98e775e862acedfe777.jpg)



Average Token Usage per Interaction(input + output tokens for multiple tool calls, without cache)


![](images/3e82831cc1415953274293be12b8935971dccebc45a9069453718fa6f5bf3700.jpg)


![](images/68f667276093cb985316d1228b70da78d73f988d1b39215da76c5d4688adc864.jpg)



Figure 13: Average tool calling success rate and token usage per interaction across different modelson CC-Bench.


Experimental Setup To evaluate the agentic coding capabilities of GLM-4.5 in real-world scenarios,we constructed CC-Bench, a benchmark built on the Claude Code5, encompassing 52 carefullydesigned programming tasks across diverse software development domains6. We compare GLM-4.5against three strong baselines: Claude Sonnet 4, Kimi K2, and Qwen3-Coder. Each task was executedin an isolated containerized environment to prevent cross-task interference, with models initializedusing predefined API configurations. Testing was conducted interactively by human experts overmultiple rounds: each task began with a standardized prompt, followed by iterative interactions where

experts adjusted inputs based on model outputs until the task was completed or failed. To ensurefairness, the same expert followed consistent interaction strategies across all models.

Based on this testing procedure, model performance was evaluated using the following criteria:the primary metric was task completion, determined by predefined completion criteria. In casesof ties, efficiency and reliability—including tool calling success rate and token consumption effi-ciency—were used as secondary metrics. The evaluation prioritized functional correctness and taskcompletion over efficiency metrics, ensuring that coding capability remained the primary evaluationfocus.

Results In head-to-head evaluations, GLM-4.5 demonstrated strong performance relative to open-source baselines and competitive capability against closed-source models as shown in figure 12.Specifically:

• GLM-4.5 vs Claude Sonnet 4: $4 0 . 4 \%$ win, $9 . 6 \%$ tie, $5 0 . 0 \%$ loss

• GLM-4.5 vs Kimi K2: $5 3 . 9 \%$ win, $1 7 . 3 \%$ tie, $2 8 . 8 \%$ loss

• GLM-4.5 vs Qwen3-Coder: $8 0 . 8 \%$ win, $7 . 7 \%$ tie, $1 1 . 5 \%$ loss

As shown in figure 13, GLM-4.5 particularly excelled in tool calling reliability, achieving the highestsuccess rate at $9 0 . 6 \%$ , compared to Claude Sonnet 4 $( 8 9 . 5 \% )$ , Kimi-K2 $( 8 6 . 2 \% )$ , and Qwen3-Coder$( 7 7 . 1 \% )$ . While Claude Sonnet 4 remains a strong competitor, GLM-4.5 outperformed other modelsin both task completion consistency and agentic execution robustness.

# 4.3.3 Evaluation of Logical Reasoning

To rigorously assess the true logical reasoning capabilities of the models and mitigate the risk ofdata contamination from common logical questions found online, we constructed a new, challengingevaluation set. This set comprises novel and complex logical reasoning problems that are structurallydifferent from those widely available on the internet. Each problem is designed to require multiplesteps of logical deduction to arrive at the correct solution.

For this evaluation, we established a unified and detailed scoring standard for each question. Wethen tasked each model with solving these problems. The correctness and quality of each model’sresponse were subsequently inspected and scored by human experts. The results show a competitivelandscape, with GLM-4.5 performing on par with leading models.


Table 11: Expert Evaluation Scores on Novel Logical Reasoning Problems


<table><tr><td>Model</td><td>Score</td></tr><tr><td>Gemini 2.5 Pro</td><td>65.8</td></tr><tr><td>DeepSeek-R1-0528</td><td>62.1</td></tr><tr><td>GLM-4.5</td><td>62.0</td></tr><tr><td>GLM-4.5-Air</td><td>53.4</td></tr><tr><td>Kimi K2</td><td>51.9</td></tr></table>

# 4.4 Evaluation of Translation

The New Paradigm of Translation Translation today extends beyond simple text conversion toencompass a nuanced understanding of evolving internet slang, cultural context, and domain-specificterminology:

Netizen Lingo: Translating “yyds” accurately requires recognizing it as the acronym for the Chinesephrase “永远的神” (yong yu ˇ an de shén), meaning “the eternal god,” thus capturing its true sentiment ˇof enthusiastic praise and admiration.

Domain Nicknames: Recognizing “胖” (literally “fat white”) is critical within photography commu-nities. Specialized models may translate it incorrectly, but a general-purpose model understands itas a widely used nickname for the “Canon EF 70-300mm f/4-5.6 IS USM” lens, providing precisetranslations.

Symbols: When a Chinese user sends a “fish” emoji in a conversation to refer to a second-handmarketplace, can the model understand the cultural meme behind it, which points to the “闲鱼”(Xiányú) platform? This tests the model’s cognitive ability to connect visual symbols with onlinecultural phenomena.

Deep Contextual Reasoning: Translating “三花公主 到，速来围观” demands identifying “三花”驾not as a person’s name but as a reference to the popular calico coloration of cats. A general-purposemodel accurately deduces this context, translating the phrase idiomatically as “The Calico Princesshas arrived! Come and see!”.

These examples underscore modern translation as a task rooted deeply in knowledge and reasoning.

Evaluation Results We tested 100 challenging, real-world cases commonly mistranslated bycurrent tools, comparing GLM-4.5 against specialized translation models (Qwen-MT-plus, Qwen-MT-turbo, Seed-X [9]) in a blind human evaluation (scored 0-3 considering whether the meaning isconveyed correctly and whether the language is authentic). The results are shown in Table 12.


Table 12: Human Scores on Selected Challenging Translation data


<table><tr><td>Model</td><td>Average Score</td></tr><tr><td>GLM-4.5</td><td>1.71</td></tr><tr><td>Qwen-MT-plus</td><td>0.38</td></tr><tr><td>Qwen-MT-turbo</td><td>0.55</td></tr><tr><td>Seed-X</td><td>0.65</td></tr></table>

GLM-4.5 significantly outperforms specialized models. For example, translating “三花公主 到”驾specialized models failed contextually, whereas GLM-4.5 accurately conveys the idiomatic meaning.

# 5 Conclusion

In this report, we have introduced the GLM-4.5 model series, including GLM-4.5 and GLM-4.5-Air.Both models adopt the MoE architecture, which improves the computational efficiency compared toprevious GLM models. GLM-4.5 excels at reasoning, coding, and agentic tasks, ranked in 3rd placeglobally among open-source and proprietary models. We release the model weights of GLM-4.5 andGLM-4.5-Air to advance the applications and research of large language models.

# 6 Contribution

Contributors’ names are listed in alphabetical order by first name. Names marked with an asterisk (*)indicate individuals who have since left our team.

# Core Contributors

Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, LucenZhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng,Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang (汪子涵), Zilin Zhu

# Contributors

Bohan Zhang, Bosi Wen, Bowen Wu, Bowen $\mathrm { X u ^ { * } }$ , Can Huang, Casey Zhao, Changpeng Cai, ChaoYu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang $\mathrm { L i ^ { * } }$ ,Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, GengzhengPan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai,Haoke Zhang, Haoran Wang, Haoyu Yang*, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu,Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang*,Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li,Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang*, Liang Xu, LinFan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao,Mingshu Zhai*, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, ShaoyouLu, Shijie Li, Shuang Li (李泷), Shuang Li (李 ), Shuxun Yang, Sibo $\mathrm { Y } _ { 1 } ^ { * }$ , Tianshu Yu, Wei Tian,爽Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang*, Xiaohan Jia,Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, XiuqingFu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, YingZhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang*, Yuhang Li,Yuhao Wu*, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, ZhenYang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang (王梓汉), Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao*

# Tech Leads

Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou

# Advisors

Jie Tang, Yuxiao Dong, Juanzi Li, Hongning Wang, Minlie Huang, Bin Xu, Jidong Zhai, WenguangChen

# Acknowledgement

We are grateful for all the support from Beijing, Shanghai, Tianjin, Hangzhou, Zhuhai, and Chengdu.Special thanks to our customers and community developers.

# References



[1] A. Abbas, K. Tirumala, D. Simig, S. Ganguli, and A. S. Morcos. Semdedup: Data-efficientlearning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023.





[2] C. An, Z. Xie, X. Li, L. Li, J. Zhang, S. Gong, M. Zhong, J. Xu, X. Qiu, M. Wang, and L. Kong.Polaris: A post-training recipe for scaling reinforcement learning on advanced reasoning models,2025.





[3] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al.Longbench: A bilingual, multitask benchmark for long context understanding. In Proceedingsof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pages 3119–3137, 2024.





[4] Y. Bai, S. Tu, J. Zhang, H. Peng, X. Wang, X. Lv, S. Cao, J. Xu, L. Hou, Y. Dong, J. Tang, andJ. Li. LongBench v2: Towards deeper understanding and reasoning on realistic long-contextmultitasks. In Proceedings of the 63rd Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 3639–3664, Vienna, Austria, July 2025. Associationfor Computational Linguistics.





[5] M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek, and M. Chen. Efficienttraining of language models to fill in the middle, 2022.





[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neuralinformation processing systems, 33:1877–1901, 2020.





[7] A. Chen, A. Li, B. Gong, B. Jiang, B. Fei, B. Yang, B. Shan, C. Yu, C. Wang, C. Zhu, et al.Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprintarXiv:2506.13585, 2025.





[8] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXivpreprint arXiv:2107.03374, 2021.





[9] S. Cheng, Y. Bao, Q. Cao, L. Huang, L. Kang, Z. Liu, Y. Lu, W. Zhu, Z. Huang, T. Li,et al. Seed-x: Building strong multilingual translation llm with 7b parameters. arXiv preprintarXiv:2507.13618, 2025.





[10] K. Deshpande, V. Sirdeshmukh, J. B. Mols, L. Jin, E.-Y. Hernandez-Cardona, D. Lee, J. Kritz,W. E. Primack, S. Yue, and C. Xing. Multichallenge: A realistic multi-turn conversation evalua-tion benchmark challenging to frontier llms. In Findings of the Association for ComputationalLinguistics: ACL 2025, pages 18632–18702, 2025.





[11] H. Ding, Z. Wang, G. Paolini, V. Kumar, A. Deoras, D. Roth, and S. Soatto. Fewer truncationsimprove language modeling. In Proceedings of the 41st International Conference on MachineLearning, pages 11030–11048, 2024.





[12] F. Gloeckle, B. Y. Idrissi, B. Rozière, D. Lopez-Paz, and G. Synnaeve. Better & faster largelanguage models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.





[13] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al.Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXivpreprint arXiv:2501.12948, 2025.





[14] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference onNeural Information Processing Systems Datasets and Benchmarks Track (Round 2).





[15] A. Henry, P. R. Dachapally, S. Pawar, and Y. Chen. Query-key normalization for transformers,2020.





[16] C.-P. Hsieh, S. Sun, S. Kriman, S. Acharya, D. Rekesh, F. Jia, and B. Ginsburg. Ruler: What’sthe real context size of your long-context language models? In First Conference on LanguageModeling.





[17] S. Hu, Y. Tu, X. Han, G. Cui, C. He, W. Zhao, X. Long, Z. Zheng, Y. Fang, Y. Huang, et al.Minicpm: Unveiling the potential of small language models with scalable training strategies. InFirst Conference on Language Modeling.





[18] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry,A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.





[19] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, andI. Stoica. Livecodebench: Holistic and contamination free evaluation of large language modelsfor code. In The Thirteenth International Conference on Learning Representations.





[20] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench:Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.





[21] K. Jordan, Y. Jin, V. Boza, Y. Jiacheng, F. Cecista, L. Newhouse, and J. Bern-stein. Muon: An optimizer for hidden layers in neural networks, 2024. URLhttps://kellerjordan.github.io/posts/muon, 6.





[22] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov. Bag of tricks for efficient text classification.In Proceedings of the 15th Conference of the European Chapter of the Association for Compu-tational Linguistics: Volume 2, Short Papers, pages 427–431. Association for ComputationalLinguistics, April 2017.





[23] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.





[24] J. Liu, J. Su, X. Yao, Z. Jiang, G. Lai, Y. Du, Y. Qin, W. Xu, E. Lu, J. Yan, et al. Muon isscalable for llm training. arXiv preprint arXiv:2502.16982, 2025.





[25] M. Luo, S. Tan, J. Wong, X. Shi, W. Y. Tang, M. Roongta, C. Cai, J. Luo, L. E. Li, R. A.Popa, and I. Stoica. Deepscaler: Surpassing o1-preview with a 1.5b model by scalingrl. https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog.





[26] S. G. Patil, H. Mao, C. Cheng-Jie Ji, F. Yan, V. Suresh, I. Stoica, and J. E. Gonzalez. Theberkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of largelanguage models. In Forty-second International Conference on Machine Learning, 2025.





[27] G. Penedo, H. Kydlícek, V. Sabol ˇ cec, B. Messmer, N. Foroutan, A. H. Kargaran, C. Raffel, ˇM. Jaggi, L. Von Werra, and T. Wolf. Fineweb2: One pipeline to scale them all–adaptingpre-training data processing to every language. arXiv preprint arXiv:2506.20920, 2025.





[28] L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, C. B. C. Zhang, M. Shaaban, J. Ling, S. Shi,et al. Humanity’s last exam. arXiv preprint arXiv:2501.14249, 2025.





[29] Y. Qin, T. Zhang, Y. Shen, W. Luo, Y. Zhang, Y. Qiao, Z. Zhou, W. Zhang, B. CUI, et al.Sysbench: Can llms follow system message? In The Thirteenth International Conference onLearning Representations, 2024.





[30] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R.Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference onLanguage Modeling, 2024.





[31] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al.Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXivpreprint arXiv:2402.03300, 2024.





[32] D. Su, K. Kong, Y. Lin, J. Jennings, B. Norick, M. Kliegl, M. Patwary, M. Shoeybi, andB. Catanzaro. Nemotron-cc: Transforming common crawl into a refined long-horizon pretrainingdataset. arXiv preprint arXiv:2412.02595, 2024.





[33] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai,A. Hauth, K. Millican, et al. Gemini: a family of highly capable multimodal models. arXivpreprint arXiv:2312.11805, 2023.





[34] K. Team, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen, et al.Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025.





[35] T. T.-B. Team. Terminal-bench: A benchmark for ai agents in terminal environments, Apr 2025.





[36] M. Tian, L. Gao, S. Zhang, X. Chen, C. Fan, X. Guo, R. Haas, P. Ji, K. Krongchon, Y. Li, et al.Scicode: A research coding benchmark curated by scientists. Advances in Neural InformationProcessing Systems, 37:30624–30650, 2024.





[37] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971, 2023.





[38] K. Vodrahalli, S. Ontanon, N. Tripuraneni, K. Xu, S. Jain, R. Shivanna, J. Hui, N. Dikkala,M. Kazemi, B. Fatemi, R. Anil, E. Dyer, S. Shakeri, R. Vij, H. Mehta, V. Ramasesh, Q. Le,E. Chi, Y. Lu, O. Firat, A. Lazaridou, J.-B. Lespiau, N. Attaluri, and K. Olszewska. Michelan-gelo: Long context evaluations beyond haystacks via latent structure queries, 2024.





[39] F. Wan, W. Shen, S. Liao, Y. Shi, C. Li, Z. Yang, J. Zhang, F. Huang, J. Zhou, and M. Yan.Qwenlong-l1: Towards long-context large reasoning models with reinforcement learning. arXivpreprint arXiv:2505.17667, 2025.





[40] L. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy formixture-of-experts. arXiv preprint arXiv:2408.15664, 2024.





[41] S. Wang, L. Yu, C. Gao, C. Zheng, S. Liu, R. Lu, K. Dang, X. Chen, J. Yang, Z. Zhang, et al.Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning forllm reasoning. arXiv preprint arXiv:2506.01939, 2025.





[42] X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, H. H.Tran, F. Li, R. Ma, M. Zheng, B. Qian, Y. Shao, N. Muennighoff, Y. Zhang, B. Hui, J. Lin,R. Brennan, H. Peng, H. Ji, and G. Neubig. Openhands: An open platform for AI softwaredevelopers as generalist agents. In The Thirteenth International Conference on LearningRepresentations, 2025.





[43] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang,et al. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.Advances in Neural Information Processing Systems, 37:95266–95290, 2024.





[44] J. Wei, N. Karina, H. W. Chung, Y. J. Jiao, S. Papay, A. Glaese, J. Schulman, and W. Fedus.Measuring short-form factuality in large language models, 2024.





[45] J. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. T. Passos, W. Fedus,and A. Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents. arXivpreprint arXiv:2504.12516, 2025.





[46] Z. Xi, Y. Ding, W. Chen, B. Hong, H. Guo, J. Wang, D. Yang, C. Liao, X. Guo, W. He, et al.Agentgym: Evolving large language model-based agents across diverse environments. arXivpreprint arXiv:2406.04151, 2024.





[47] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, et al.Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.





[48] S. Yao, N. Shinn, P. Razavi, and K. Narasimhan. tau-bench: A benchmark for tool-agent-userinteraction in real-world domains. arXiv preprint arXiv:2406.12045, 2024.





[49] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, W. Dai, T. Fan, G. Liu, L. Liu, et al. Dapo:An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476,2025.





[50] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al.Glm-130b: An open bilingual pre-trained model. In The Eleventh International Conference onLearning Representations.





[51] Z. Zhang, L. Lei, L. Wu, R. Sun, Y. Huang, C. Long, X. Liu, X. Lei, J. Tang, and M. Huang.Safetybench: Evaluating the safety of large language models with multiple choice questions.arXiv preprint arXiv:2309.07045, 2023.





[52] J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.

